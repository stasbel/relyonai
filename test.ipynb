{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "# logging.basicConfig(level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiknows import ai, gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.prompt.prompt:# user\n",
      "TASK: \"\"\"\n",
      "read file in path `file`\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: [file]\n",
      "- file: \"\"\"\n",
      "object of type builtins.str\n",
      "\"\"\"\n",
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 56\n",
      "INFO:aiknows.ai:estimate prompt token length: 3077\n",
      "INFO:aiknows.ai:real prompt token length: 3484\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "# 'file' is given via args, so we use it to construct the path\n",
      "try:\n",
      "    with open(file, 'r') as f:\n",
      "        content = f.read()\n",
      "except FileNotFoundError:\n",
      "    finish_task_error(f'File {file} not found', error_cause=True)\n",
      "else:\n",
      "    finish_task_ok(content)\n",
      "```\n"
     ]
    },
    {
     "ename": "FinishTaskErrorSignal",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFinishTaskErrorSignal\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ai(\u001b[39m'\u001b[39;49m\u001b[39mread file in path `file`\u001b[39;49m\u001b[39m'\u001b[39;49m, file\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/file/path\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/ai.py:107\u001b[0m, in \u001b[0;36mai\u001b[0;34m(task, save_session, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m runtime\u001b[39m.\u001b[39madd_vars(kwargs)\n\u001b[1;32m    105\u001b[0m session \u001b[39m=\u001b[39m Session(chat, runtime)\n\u001b[0;32m--> 107\u001b[0m result \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39;49mai(task, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    109\u001b[0m \u001b[39mif\u001b[39;00m save_session:\n\u001b[1;32m    110\u001b[0m     \u001b[39mreturn\u001b[39;00m result, session\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/ai.py:76\u001b[0m, in \u001b[0;36mSession.ai\u001b[0;34m(self, task, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, ak_runtime\u001b[39m.\u001b[39mFinishTaskErrorSignal):\n\u001b[1;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39merror_cause:\n\u001b[0;32m---> 76\u001b[0m         \u001b[39mraise\u001b[39;00m e \u001b[39mfrom\u001b[39;00m \u001b[39mlast_error\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m         \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/ai.py:65\u001b[0m, in \u001b[0;36mSession.ai\u001b[0;34m(self, task, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[39m# chat is common to not follow rules all the time\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     code \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat\u001b[39m.\u001b[39mstrip_code_markdown(code)\n\u001b[0;32m---> 65\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mruntime\u001b[39m.\u001b[39;49mrun(code)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat\u001b[39m.\u001b[39madd_assistant(code)\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/runtime.py:199\u001b[0m, in \u001b[0;36mLocalRuntime.run\u001b[0;34m(self, code, supress_stdout)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mwith\u001b[39;00m contextlib\u001b[39m.\u001b[39mredirect_stdout(\u001b[39mNone\u001b[39;00m) \u001b[39mif\u001b[39;00m supress_stdout \u001b[39melse\u001b[39;00m contextlib\u001b[39m.\u001b[39mnullcontext():\n\u001b[1;32m    198\u001b[0m     \u001b[39mwith\u001b[39;00m enable_signals():\n\u001b[0;32m--> 199\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_jupyter_style(code)\n\u001b[1;32m    201\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobals[\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m result\n\u001b[1;32m    202\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/runtime.py:193\u001b[0m, in \u001b[0;36mLocalRuntime._execute_jupyter_style\u001b[0;34m(self, code)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m     \u001b[39m# If the last body element ends with semicolon, do not return its value\u001b[39;00m\n\u001b[1;32m    192\u001b[0m     last \u001b[39m=\u001b[39m ast\u001b[39m.\u001b[39mModule(body\u001b[39m=\u001b[39m[last], type_ignores\u001b[39m=\u001b[39m[])\n\u001b[0;32m--> 193\u001b[0m     exec(\u001b[39mcompile\u001b[39;49m(last, filename\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mEXEC_FILENAME, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mexec\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mglobals)\n\u001b[1;32m    194\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m<gpt>:6\u001b[0m\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/runtime.py:143\u001b[0m, in \u001b[0;36mfinish_task_error\u001b[0;34m(message, error_cause)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfinish_task_error\u001b[39m(message: \u001b[39mstr\u001b[39m, error_cause: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     \u001b[39mif\u001b[39;00m _can_throw_signals:\n\u001b[0;32m--> 143\u001b[0m         \u001b[39mraise\u001b[39;00m FinishTaskErrorSignal(message, error_cause)\n",
      "\u001b[0;31mFinishTaskErrorSignal\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ai('read file in path `file`', file='/file/path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.prompt.prompt:# user\n",
      "TASK: \"\"\"\n",
      "a list of non primes in list_x\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: [list_x]\n",
      "- list_x: \"\"\"\n",
      "object of type builtins.list\n",
      "\"\"\"\n",
      "INFO:aiknows.llm:cache miss, generating bytecode\n",
      "INFO:aiknows.llm:len prompt messages: 56\n",
      "INFO:aiknows.llm:estimate prompt token length: 3089\n",
      "INFO:aiknows.llm:real prompt token length: 3528\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "# we need a function to check if a number is prime\n",
      "def is_prime(n):\n",
      "    if n < 2:\n",
      "        return False\n",
      "    for i in range(2, int(n ** 0.5) + 1):\n",
      "        if n % i == 0:\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "# we can use filter to get non-prime numbers\n",
      "result = list(filter(lambda x: not is_prime(x), list_x))\n",
      "finish_task_ok(result);\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 4, 6, 8, 9]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('a list of non primes in list_x', list_x=[1, 2, 3, 4, 5, 6, 7, 8, 9]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.prompt.prompt:# user\n",
      "TASK: \"\"\"\n",
      "filter non primes out\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: [list_x]\n",
      "- list_x: \"\"\"\n",
      "object of type builtins.list\n",
      "\"\"\"\n",
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 56\n",
      "INFO:aiknows.ai:estimate prompt token length: 3035\n",
      "INFO:aiknows.ai:real prompt token length: 3427\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "# we assume that by filtering user meant keeping elements satisfying some condition\n",
      "# description is too vague, so we proceed with the most common pythonic way -- list comprehension\n",
      "# input list is provided via args, so we create a function for a result\n",
      "def is_prime(n):\n",
      "    if n < 2:\n",
      "        return False\n",
      "    for i in range(2, int(n ** 0.5) + 1):\n",
      "        if n % i == 0:\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "def filter_primes(xs):\n",
      "    return [x for x in xs if is_prime(x)]\n",
      "\n",
      "finish_task_ok(filter_primes(list_x));\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 3, 5, 7]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('filter non primes out', list_x=[1, 2, 3, 4, 5, 6, 7, 8, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.prompt.prompt:# user\n",
      "TASK: \"\"\"\n",
      "sum numbers in x\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: [x]\n",
      "- x: \"\"\"\n",
      "object of type builtins.list\n",
      "\"\"\"\n",
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 56\n",
      "INFO:aiknows.ai:estimate prompt token length: 3011\n",
      "INFO:aiknows.ai:real prompt token length: 3289\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "# x list is available via args\n",
      "result = sum(x)\n",
      "finish_task_ok(result);\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r, sess = ai('sum numbers in x', x=[1,2,3,4,5], save_session=True)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.prompt.prompt:# user\n",
      "TASK: \"\"\"\n",
      "multiply numbers in x\n",
      "\"\"\"\n",
      "REUSE: True\n",
      "ARGS: []\n",
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 58\n",
      "INFO:aiknows.ai:estimate prompt token length: 3051\n",
      "INFO:aiknows.ai:real prompt token length: 3386\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "# x list is not available via args, so we create a function\n",
      "def multiply_numbers(xs):\n",
      "    result = 1\n",
      "    for x in xs:\n",
      "        result *= x\n",
      "    return result\n",
      "\n",
      "# reuse flag is set, so we can reference previously defined function\n",
      "result = multiply_numbers(x)\n",
      "finish_task_ok(result);\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.ai('multiply numbers in x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.prompt.prompt:# user\n",
      "TASK: \"\"\"\n",
      "keep the non-primes in l\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: [l]\n",
      "- l: \"\"\"\n",
      "object of type builtins.list\n",
      "\"\"\"\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "# we assume that by \"keep non-primes\" user meant \"remove primes\"\n",
      "# we can use sympy package to check for primes\n",
      "from sympy import isprime\n",
      "\n",
      "# input list is given via args, so we use it\n",
      "def remove_primes(xs):\n",
      "    return [x for x in xs if not isprime(x)]\n",
      "\n",
      "finish_task_ok(remove_primes(l));\n",
      "```\n",
      "INFO:aiknows.prompt.prompt:# user\n",
      "ERROR: \"\"\"\n",
      "File \"<gpt>\", line 3, in <module>\n",
      "ModuleNotFoundError: No module named 'sympy'\n",
      "\"\"\"\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "# sympy package is missing, so we can't proceed with the task\n",
      "finish_task_error('sympy package isn\\'t installed', error_cause=True);\n",
      "```\n"
     ]
    },
    {
     "ename": "FinishTaskErrorSignal",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/src/aiknows/aiknows/ai.py:65\u001b[0m, in \u001b[0;36mSession.ai\u001b[0;34m(self, task, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m     code \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat\u001b[39m.\u001b[39mstrip_code_markdown(code)\n\u001b[0;32m---> 65\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mruntime\u001b[39m.\u001b[39;49mrun(code)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/runtime.py:199\u001b[0m, in \u001b[0;36mLocalRuntime.run\u001b[0;34m(self, code, supress_stdout)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[39mwith\u001b[39;00m enable_signals():\n\u001b[0;32m--> 199\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_jupyter_style(code)\n\u001b[1;32m    201\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobals[\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m result\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/runtime.py:184\u001b[0m, in \u001b[0;36mLocalRuntime._execute_jupyter_style\u001b[0;34m(self, code)\u001b[0m\n\u001b[1;32m    182\u001b[0m tree\u001b[39m.\u001b[39mbody \u001b[39m=\u001b[39m tree\u001b[39m.\u001b[39mbody[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 184\u001b[0m exec(\u001b[39mcompile\u001b[39;49m(tree, filename\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mEXEC_FILENAME, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mexec\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mglobals)\n\u001b[1;32m    186\u001b[0m \u001b[39mif\u001b[39;00m return_last_expr \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(last, ast\u001b[39m.\u001b[39mExpr):\n\u001b[1;32m    187\u001b[0m     \u001b[39m# If the last body element does not end with semicolon, return its value\u001b[39;00m\n",
      "File \u001b[0;32m<gpt>:3\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sympy'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFinishTaskErrorSignal\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ai(\u001b[39m'\u001b[39;49m\u001b[39mkeep the non-primes in l\u001b[39;49m\u001b[39m'\u001b[39;49m, l\u001b[39m=\u001b[39;49m[\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m4\u001b[39;49m, \u001b[39m5\u001b[39;49m, \u001b[39m6\u001b[39;49m, \u001b[39m7\u001b[39;49m, \u001b[39m8\u001b[39;49m, \u001b[39m9\u001b[39;49m])\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/ai.py:107\u001b[0m, in \u001b[0;36mai\u001b[0;34m(task, save_session, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m runtime\u001b[39m.\u001b[39madd_vars(kwargs)\n\u001b[1;32m    105\u001b[0m session \u001b[39m=\u001b[39m Session(chat, runtime)\n\u001b[0;32m--> 107\u001b[0m result \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39;49mai(task, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    109\u001b[0m \u001b[39mif\u001b[39;00m save_session:\n\u001b[1;32m    110\u001b[0m     \u001b[39mreturn\u001b[39;00m result, session\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/ai.py:76\u001b[0m, in \u001b[0;36mSession.ai\u001b[0;34m(self, task, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, ak_runtime\u001b[39m.\u001b[39mFinishTaskErrorSignal):\n\u001b[1;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39merror_cause:\n\u001b[0;32m---> 76\u001b[0m         \u001b[39mraise\u001b[39;00m e \u001b[39mfrom\u001b[39;00m \u001b[39mlast_error\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m         \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/ai.py:65\u001b[0m, in \u001b[0;36mSession.ai\u001b[0;34m(self, task, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[39m# chat is common to not follow rules all the time\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     code \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat\u001b[39m.\u001b[39mstrip_code_markdown(code)\n\u001b[0;32m---> 65\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mruntime\u001b[39m.\u001b[39;49mrun(code)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat\u001b[39m.\u001b[39madd_assistant(code)\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/runtime.py:199\u001b[0m, in \u001b[0;36mLocalRuntime.run\u001b[0;34m(self, code, supress_stdout)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mwith\u001b[39;00m contextlib\u001b[39m.\u001b[39mredirect_stdout(\u001b[39mNone\u001b[39;00m) \u001b[39mif\u001b[39;00m supress_stdout \u001b[39melse\u001b[39;00m contextlib\u001b[39m.\u001b[39mnullcontext():\n\u001b[1;32m    198\u001b[0m     \u001b[39mwith\u001b[39;00m enable_signals():\n\u001b[0;32m--> 199\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_jupyter_style(code)\n\u001b[1;32m    201\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobals[\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m result\n\u001b[1;32m    202\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/runtime.py:193\u001b[0m, in \u001b[0;36mLocalRuntime._execute_jupyter_style\u001b[0;34m(self, code)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m     \u001b[39m# If the last body element ends with semicolon, do not return its value\u001b[39;00m\n\u001b[1;32m    192\u001b[0m     last \u001b[39m=\u001b[39m ast\u001b[39m.\u001b[39mModule(body\u001b[39m=\u001b[39m[last], type_ignores\u001b[39m=\u001b[39m[])\n\u001b[0;32m--> 193\u001b[0m     exec(\u001b[39mcompile\u001b[39;49m(last, filename\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mEXEC_FILENAME, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mexec\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mglobals)\n\u001b[1;32m    194\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m<gpt>:2\u001b[0m\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/runtime.py:143\u001b[0m, in \u001b[0;36mfinish_task_error\u001b[0;34m(message, error_cause)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfinish_task_error\u001b[39m(message: \u001b[39mstr\u001b[39m, error_cause: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     \u001b[39mif\u001b[39;00m _can_throw_signals:\n\u001b[0;32m--> 143\u001b[0m         \u001b[39mraise\u001b[39;00m FinishTaskErrorSignal(message, error_cause)\n",
      "\u001b[0;31mFinishTaskErrorSignal\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ai('keep the non-primes in l', l=[1, 2, 3, 4, 5, 6, 7, 8, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.prompt.prompt:# user\n",
      "TASK: \"\"\"\n",
      "module for handy higher-order functions\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: []\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "# python standard library has functools module that provides higher-order functions\n",
      "import functools\n",
      "finish_task_ok(functools);\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'functools' from '/Users/stanb/.miniconda3/envs/aiknows/lib/python3.8/functools.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('module for handy higher-order functions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiknows import ai, gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.prompt.prompt:# user\n",
      "TASK: \"\"\"\n",
      "get the current time in HH:mm:ss format in UTC\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: []\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "# datetime module is part of python standard library\n",
      "from datetime import datetime\n",
      "\n",
      "# get current time in UTC timezone\n",
      "now = datetime.utcnow()\n",
      "\n",
      "# format time as HH:mm:ss\n",
      "time_str = now.strftime('%H:%M:%S')\n",
      "\n",
      "finish_task_ok(time_str);\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'02:00:05'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('get the current time in HH:mm:ss format in UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.prompt.prompt:# user\n",
      "TASK: \"\"\"\n",
      "module is for higher-order functions\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: []\n",
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 52\n",
      "INFO:aiknows.ai:estimate prompt token length: 2786\n",
      "INFO:aiknows.ai:real prompt token length: 3050\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "# higher-order functions are a part of python standard library\n",
      "import functools\n",
      "finish_task_ok(functools);\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'functools' from '/Users/stanb/.miniconda3/envs/aiknows/lib/python3.8/functools.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('module is for higher-order functions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.prompt.prompt:# user\n",
      "TASK: \"\"\"\n",
      "calculate number of files in current directory\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: []\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "import os\n",
      "\n",
      "# we assume that user wants to get a function that does the counting\n",
      "def count_files_in_current_directory():\n",
      "    return len(os.listdir('.'))\n",
      "\n",
      "finish_task_ok(count_files_in_current_directory);\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function count_files_in_current_directory()>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('calculate number of files in current directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.prompt.prompt:# user\n",
      "TASK: \"\"\"\n",
      "keep the non-primes in l\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: [l]\n",
      "- l: \"\"\"\n",
      "object of type builtins.list\n",
      "\"\"\"\n",
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 48\n",
      "INFO:aiknows.ai:estimate prompt token length: 2701\n",
      "INFO:aiknows.ai:real prompt token length: 3052\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "# separate function for condition is convenient here\n",
      "def is_prime(n):\n",
      "    if n < 2:\n",
      "        return False\n",
      "    for i in range(2, int(n ** 0.5) + 1):\n",
      "        if n % i == 0:\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "# input list is given via args, so we use it\n",
      "# in the context of task description, we assume that user meant keeping elements for which condition is false\n",
      "def keep_non_primes(xs):\n",
      "    return [x for x in xs if not is_prime(x)]\n",
      "\n",
      "finish_task_ok(keep_non_primes(l));\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 4, 6, 8, 9]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('keep the non-primes in l', l=[1, 2, 3, 4, 5, 6, 7, 8, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.prompt.prompt:# user\n",
      "TASK: \"\"\"\n",
      "keep the non-primes\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: [l]\n",
      "- l: \"\"\"\n",
      "object of type builtins.list\n",
      "\"\"\"\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "# we need a function to check if a number is prime\n",
      "def is_prime(n):\n",
      "    if n < 2:\n",
      "        return False\n",
      "    for i in range(2, int(n ** 0.5) + 1):\n",
      "        if n % i == 0:\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "# input list is given, so we use it to create a function for a result\n",
      "def keep_non_primes(xs):\n",
      "    return [x for x in xs if not is_prime(x)]\n",
      "\n",
      "finish_task_ok(keep_non_primes);\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function keep_non_primes(xs)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('keep the non-primes', l=[1, 2, 3, 4, 5, 6, 7, 8, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.prompt.prompt:# user\n",
      "TASK: \"\"\"\n",
      "sum of 1 and x\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: [x]\n",
      "- x: \"\"\"\n",
      "object of type builtins.int\n",
      "\"\"\"\n",
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 48\n",
      "INFO:aiknows.ai:estimate prompt token length: 2700\n",
      "INFO:aiknows.ai:real prompt token length: 2946\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "# x is given, so we just add 1 to it\n",
      "finish_task_ok(1 + x);\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('sum of 1 and x', x=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.prompt.prompt:# user\n",
      "TASK: \"\"\"\n",
      "keep the non-primes\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: [l]\n",
      "- l: \"\"\"\n",
      "object of type builtins.list\n",
      "\"\"\"\n",
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 44\n",
      "INFO:aiknows.ai:estimate prompt token length: 2571\n",
      "INFO:aiknows.ai:real prompt token length: 2912\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "# we assume that by keeping non-primes user meant keeping elements satisfying some condition\n",
      "# description is too vague, so we proceed with the most common pythonic way -- list comprehension\n",
      "# input list is provided via args, so we use it directly\n",
      "def is_prime(n):\n",
      "    if n < 2:\n",
      "        return False\n",
      "    for i in range(2, int(n ** 0.5) + 1):\n",
      "        if n % i == 0:\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "def keep_non_primes(xs):\n",
      "    return [x for x in xs if not is_prime(x)]\n",
      "\n",
      "finish_task_ok(keep_non_primes(l));\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 4, 6, 8, 9]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('keep the non-primes', l=[1, 2, 3, 4, 5, 6, 7, 8, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiknows import ai, gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# system\n",
      "\n",
      "# python code assistant\n",
      "\n",
      "## description\n",
      "\n",
      "You are a helpful proficient code generator assistant who only respond with a python3.8 compatible code snippets. User giving you a task in natural language and responds back by intepreting your code on a real python3.8 interpretator. You code should end up with a python object that solves the user's task.\n",
      "\n",
      "## user tasks\n",
      "\n",
      "- User initiates a new task by sending a message starting with 'TASK: \"\"\"{task}\"\"\"' where {task} is a natural language description of the task for assistant to solve. You should understand the task objective and proceed to solving in the the most sensible way.\n",
      "- Following in the same message is a reuse flag in format 'REUSE: {reuse}' where {reuse} is a boolean value indicating whether python global namespace in cleaned or reused from previous task. If flag is unset, then user's intepretator is restarted and namespace is cleared. If flag is set, nothing is done with user's intepretator, so you can continue to reference global variables from previous restart.\n",
      "- Following in the same message is a list of arguments in format 'ARGS: [{args}]\\n{explanations}' where {args} is a comma separated list of arguments names and {explanations} is a list of informations about these arguments (mostly types). You can reference these arguments direcly in your code as global variables.\n",
      "\n",
      "## assistant messages\n",
      "\n",
      "- Your responses are valid markdown code blocks from start to finish: starts with \"```python\" and ends with \"```\". Between them you can only put valid python3.8 code.\n",
      "- Think out loud and step by step: use long comments in code and the code itself to reason about your desicions.\n",
      "- Two functions are available as global variables for signaling task finishing in your code snippets:\n",
      "    - `finish_task_ok(result: Any, message: str | None = None) -> None` - for successful task finishing and specifing resulting python object which solves the task. If task objectve is an outside effect (e.g. sending a message), then `result` should be `None`.\n",
      "    - `finish_task_error(message: str, error_cause: bool) -> None` - for unsuccessful task finishing and specifing error message explanining the reason of inability to get result.\n",
      "\n",
      "## user responses\n",
      "\n",
      "- User responds back by intepreting code snippets on a real python3.8 intepretator:\n",
      "    - If code is valid, user will respond with a 'RESULT: \"\"\"{result}\"\"\"' message, where {result} is a `repr` of last expression in your code snippet. Use it for printing out intermediate results that helps in a reasoning for next steps.\n",
      "    - If code is invalid, user will respond with an 'ERROR: \"\"\"{error}\"\"\"' message, where {error} is a traceback of python3.8 error. Use it for debugging your code. If error occurs, fix your code in the next snippet to work properly.\n",
      "\n",
      "## `gpt` function for text processing\n",
      "\n",
      "- Unless specifically outlined in task description, user usually refers to Chat-GPT in anything related to text processing:\n",
      "    - summarization, rewriting, grammar correction, shortening, expanding, etc.\n",
      "    - thesaurus, dictionary, translation, etc.\n",
      "    - ideation, facts, comparison, etc.\n",
      "- For this, global function `gpt(prompt: str, *, t: float = 1.0) -> str` is provided to use in code snippets.\n",
      "\n",
      "\n",
      "# user\n",
      "\n",
      "TASK: \"\"\"\n",
      "that package for dataframes manipulation and stuff\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: []\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "finish_task_ok(pd);\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "TASK: \"\"\"\n",
      "number of seconds in 10 years\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: []\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "# seconds x minutes x hours x days x years\n",
      "finish_task_ok(60 * 60 * 24 * 365 * 10);\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "TASK: \"\"\"\n",
      "get a result value from single key dictionary d\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: [d]\n",
      "- d: \"\"\"\n",
      "object of type builtins.dict\n",
      "\"\"\"\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "# we don't know the right key name, could be: response, body, result, etc.\n",
      "# no problem, let's print them out and explore\n",
      "d.keys()\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "RESULT: \"\"\"\n",
      "dict_keys(['res', 'query', 'flag'])\n",
      "\"\"\"\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "# only 3 of them and 'res' makes the most sense in task context\n",
      "finish_task_ok(d['res']);\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "TASK: \"\"\"\n",
      "read file filename.txt\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: [filename]\n",
      "- filename: \"\"\"\n",
      "object of type builtins.str\n",
      "\"\"\"\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "# 'filename' is one of the task args, so we use it to construct the path\n",
      "# the most sensible way will be is to assume following structure\n",
      "with open(f'{filename}.txt') as f:\n",
      "    content = f.read()\n",
      "\n",
      "finish_task_ok(content);\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "TASK: \"\"\"\n",
      "read a file\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: []\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "# the task has no args that could be assumed to be file paths\n",
      "# so we construct a function that takes a file path as input\n",
      "def read_file(file_path):\n",
      "    with open(file_path) as f:\n",
      "        content = f.read()\n",
      "    return content\n",
      "\n",
      "finish_task_ok(read_file);\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "TASK: \"\"\"\n",
      "read file filename.txt\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: []\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "# the task has no file args passed in, but task mentioned 'filename.txt'\n",
      "# the most sensible way would be to assume 'filename.txt' is the file path\n",
      "with open(f'filename.txt') as f:\n",
      "    content = f.read()\n",
      "\n",
      "finish_task_ok(content);\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "TASK: \"\"\"\n",
      "make line plot of `a` with specified color\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: [a]\n",
      "- a: \"\"\"\n",
      "object of type numpy.ndarray\n",
      "array of shape (10,) with dtype float64\n",
      "\"\"\"\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "# packages might be missing, but we don't attempt to install them\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# color isn't provided but mentioned so we made a func\n",
      "def plot_line(color):\n",
      "    # generate x and y values\n",
      "    # a is provided so we just reference it directly\n",
      "    n = a.shape[0]\n",
      "    x = np.linspace(0, n - 1, num=n)\n",
      "    y = a\n",
      "\n",
      "    # create a line plot using the specified color\n",
      "    fig = plt.figure()\n",
      "    plt.plot(x, y, color=color)\n",
      "\n",
      "    # add known labels and title to the plot\n",
      "    plt.ylabel('a')\n",
      "    plt.title(f'line plot of a with {color} color')\n",
      "\n",
      "    # user don't intend to observe outside effect\n",
      "    plt.close(fig)\n",
      "\n",
      "    return fig\n",
      "\n",
      "finish_task_ok(plot_line);\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "TASK: \"\"\"\n",
      "read filename.txt and translate it to french\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: []\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "# the task has no file args passed in, but task mentioned 'filename.txt'\n",
      "# the most sensible way would be to assume 'filename.txt' is the file path\n",
      "with open(f'filename.txt') as f:\n",
      "    content = f.read()\n",
      "\n",
      "# task mentioned generic translation, that's what chat-gpt good at\n",
      "# so we invoke it from the code with global `gpt` func\n",
      "content_fr = gpt(f'Translate the following text to French: \"\"\"\\n{content}\\n\"\"\"')\n",
      "\n",
      "# let's have a quick peek at the result\n",
      "content, content_fr\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "RESULT: \"\"\"\n",
      "('filename content', '\"Contenu du nom de fichier\"')\n",
      "\"\"\"\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "# this seems about right, we can refer to last result object with '_'\n",
      "finish_task_ok(_[1]);\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "TASK: \"\"\"\n",
      "creat a sum of a and b\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: [a, b]\n",
      "- a: \"\"\"\n",
      "object of type builtins.list\n",
      "\"\"\"\n",
      "- b: \"\"\"\n",
      "object of type builtins.dict\n",
      "\"\"\"\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "finish_task_error('unable to create a sum of a list and a dict');\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "TASK: \"\"\"\n",
      "read file notexists.txt\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: []\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "with open('notexists.txt', 'r') as f:\n",
      "    content = f.read()\n",
      "\n",
      "finish_task_ok(content);\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "ERROR: \"\"\"\n",
      "File \"<gpt>\", line 1, in <module>\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'notexists.txt'\n",
      "\"\"\"\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "# last exception is direct cause or innability to complete task, so we mark it with flag\n",
      "finish_task_error('file from task description isn\\'t exists', error_cause=True);\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "TASK: \"\"\"\n",
      "filter words beginning with \"a\"\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: []\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "# we assume that by filtering user meant filering from the list\n",
      "# list isn't provided so we return a function that applies to it\n",
      "def filter(words):\n",
      "    return [w for w in words if w.startswith('a')]\n",
      "\n",
      "finish_task_ok(filter);\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "TASK: \"\"\"\n",
      "merge two lists into a single one\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: []\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "# lists isn't provided via args\n",
      "# so we return a function that takes two lists and merges them\n",
      "def merge(a, b):\n",
      "    return a + b\n",
      "\n",
      "finish_task_ok(merge);\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "TASK: \"\"\"\n",
      "given the list of persons, return their combined age\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: [persons]\n",
      "- persons: \"\"\"\n",
      "object of type builtins.list\n",
      "\"\"\"\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "# persons list is given, assuming each person is an object of some common structure\n",
      "# let's explore it:\n",
      "type(persons[0])\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "RESULT: \"\"\"\n",
      "<class 'dict'>\n",
      "\"\"\"\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "# aha, a plain dict, let's see what keys are there:\n",
      "persons[0].keys()\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "RESULT: \"\"\"\n",
      "dict_keys(['name', 'age (y)', 'id'])\n",
      "\"\"\"\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "# key 'age (y)' looks like what we need\n",
      "combined_age = sum([p['age (y)'] for p in persons])\n",
      "finish_task_ok(combined_age);\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "TASK: \"\"\"\n",
      "return number of males among persons\n",
      "\"\"\"\n",
      "REUSE: True\n",
      "ARGS: []\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "# persons list is given as namespace is reused from previous task\n",
      "# we already know that each person is a dict with 'name' key\n",
      "# let gpt determine gender by name\n",
      "\n",
      "GPT_GENDER_PROMPT = 'print out single digit -- 1 if following name is of man, 0 if is of woman'\n",
      "\n",
      "total_males = 0\n",
      "for person in persons:\n",
      "    name = person['name']\n",
      "    total_males += int(gpt(f'{GPT_GENDER_PROMPT}: \"{name}\"'))\n",
      "\n",
      "finish_task_ok(total_males);\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "TASK: \"\"\"\n",
      "read file\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: [file]\n",
      "- file: \"\"\"\n",
      "object of type builtins.str\n",
      "\"\"\"\n",
      "\n"
     ]
    },
    {
     "ename": "FinishTaskErrorSignal",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/src/aiknows/aiknows/ai.py:60\u001b[0m, in \u001b[0;36mai\u001b[0;34m(task, save_runtime, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[39m# we don't redirect stdout/stderr as we want to feel as natural as possible\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     result \u001b[39m=\u001b[39m local_runtime\u001b[39m.\u001b[39;49mrun(code)\n\u001b[1;32m     61\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/runtime.py:195\u001b[0m, in \u001b[0;36mLocalRuntime.run\u001b[0;34m(self, code, supress_stdout)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m contextlib\u001b[39m.\u001b[39mredirect_stdout(\u001b[39mNone\u001b[39;00m) \u001b[39mif\u001b[39;00m supress_stdout \u001b[39melse\u001b[39;00m contextlib\u001b[39m.\u001b[39mnullcontext():\n\u001b[0;32m--> 195\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_jupyter_style(code)\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/runtime.py:189\u001b[0m, in \u001b[0;36mLocalRuntime._execute_jupyter_style\u001b[0;34m(self, code)\u001b[0m\n\u001b[1;32m    188\u001b[0m last \u001b[39m=\u001b[39m ast\u001b[39m.\u001b[39mModule(body\u001b[39m=\u001b[39m[last], type_ignores\u001b[39m=\u001b[39m[])\n\u001b[0;32m--> 189\u001b[0m exec(\u001b[39mcompile\u001b[39;49m(last, filename\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mEXEC_FILENAME, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mexec\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mglobals)\n\u001b[1;32m    190\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m<gpt>:8\u001b[0m\n",
      "File \u001b[0;32m<gpt>:4\u001b[0m, in \u001b[0;36mread_file\u001b[0;34m(file_path)\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kek/mem'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFinishTaskErrorSignal\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ai(\u001b[39m'\u001b[39;49m\u001b[39mread file\u001b[39;49m\u001b[39m'\u001b[39;49m, file\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/kek/mem\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/ai.py:71\u001b[0m, in \u001b[0;36mai\u001b[0;34m(task, save_runtime, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, runtime\u001b[39m.\u001b[39mFinishTaskErrorSignal):\n\u001b[1;32m     70\u001b[0m     \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39merror_cause:\n\u001b[0;32m---> 71\u001b[0m         \u001b[39mraise\u001b[39;00m e \u001b[39mfrom\u001b[39;00m \u001b[39mlast_error\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m         \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/ai.py:60\u001b[0m, in \u001b[0;36mai\u001b[0;34m(task, save_runtime, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m code \u001b[39m=\u001b[39m _generate_or_retrieve_code(chat\u001b[39m.\u001b[39mmessages)\n\u001b[1;32m     58\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[39m# we don't redirect stdout/stderr as we want to feel as natural as possible\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     result \u001b[39m=\u001b[39m local_runtime\u001b[39m.\u001b[39;49mrun(code)\n\u001b[1;32m     61\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     62\u001b[0m     chat\u001b[39m.\u001b[39madd_assistant(code)\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/runtime.py:195\u001b[0m, in \u001b[0;36mLocalRuntime.run\u001b[0;34m(self, code, supress_stdout)\u001b[0m\n\u001b[1;32m    193\u001b[0m code \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strip_markdown(code)\n\u001b[1;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m contextlib\u001b[39m.\u001b[39mredirect_stdout(\u001b[39mNone\u001b[39;00m) \u001b[39mif\u001b[39;00m supress_stdout \u001b[39melse\u001b[39;00m contextlib\u001b[39m.\u001b[39mnullcontext():\n\u001b[0;32m--> 195\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_jupyter_style(code)\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/runtime.py:189\u001b[0m, in \u001b[0;36mLocalRuntime._execute_jupyter_style\u001b[0;34m(self, code)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[39m# If the last body element ends with semicolon, do not return its value\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     last \u001b[39m=\u001b[39m ast\u001b[39m.\u001b[39mModule(body\u001b[39m=\u001b[39m[last], type_ignores\u001b[39m=\u001b[39m[])\n\u001b[0;32m--> 189\u001b[0m     exec(\u001b[39mcompile\u001b[39;49m(last, filename\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mEXEC_FILENAME, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mexec\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mglobals)\n\u001b[1;32m    190\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m<gpt>:2\u001b[0m\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/runtime.py:129\u001b[0m, in \u001b[0;36mfinish_task_error\u001b[0;34m(message, error_cause)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfinish_task_error\u001b[39m(message: \u001b[39mstr\u001b[39m, error_cause: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[39mif\u001b[39;00m throw_signals:\n\u001b[0;32m--> 129\u001b[0m         \u001b[39mraise\u001b[39;00m FinishTaskErrorSignal(message, error_cause)\n",
      "\u001b[0;31mFinishTaskErrorSignal\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ai('read file', file='/kek/mem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# system\n",
      "\n",
      "# python code assistant\n",
      "\n",
      "## description\n",
      "\n",
      "You are a helpful proficient code generator assistant who only respond with a python3.8 compatible code snippets. User giving you a task in natural language and responds back by intepreting your code on a real python3.8 interpretator. You code should end up with a python object that solves the user's task.\n",
      "\n",
      "## user tasks\n",
      "\n",
      "- User initiates a new task by sending a message starting with 'TASK: \"\"\"{task}\"\"\"' where {task} is a natural language description of the task for assistant to solve.\n",
      "- Following in the same message is a reuse flag in format 'REUSE: {reuse}' where {reuse} is a boolean value indicating whether python global namespace in cleaned or reused from previous task. If flag is unset, then user's intepretator is restarted and namespace is cleared. If flag is set, nothing is done with user's intepretator, so you can continue to reference global variables from previous restart.\n",
      "- Following in the same message is a list of arguments in format 'ARGS: [{args}]\\n{explanations}' where {args} is a comma separated list of arguments names and {explanations} is a list of informations about these arguments (mostly types). You can reference these arguments direcly in your code as global variables.\n",
      "\n",
      "## assistant messages\n",
      "\n",
      "- Your responses starts with \"```python\" and ends with \"```\". Between them you can only put valid python3.8 code.\n",
      "- Think out loud and step by step: use long comments in code and the code itself to reason about your desicions.\n",
      "- Two functions are available as global variables for signaling task ending in your code snippets:\n",
      "    - `finish_task_ok(result: Any, message: str | None = None) -> None` - for successful task finishing and specifing resulting python object which solves the task.\n",
      "    - `finish_task_error(message: str, error_cause: bool) -> None` - for unsuccessful task finishing and specifing error message explanining inability reason.\n",
      "\n",
      "## user responses\n",
      "\n",
      "- User responds back by intepreting code snippets on a real python3.8 intepretator:\n",
      "    - If code is valid, user will respond with a 'RESULT: \"\"\"{result}\"\"\"' message, where {result} is a `repr` of last expression in your code snippet. Use it for printing out intermediate results that helps in a reasoning for next steps.\n",
      "    - If code is invalid, user will respond with an 'ERROR: \"\"\"{error}\"\"\"' message, where {error} is a traceback of python3.8 error. Use it for debugging your code. If error occurs, fix your code in the next snippet to work properly.\n",
      "\n",
      "## `gpt` function for text processing\n",
      "\n",
      "- Unless specifically outlined in task description, user usually refers to Chat-GPT in anything related to text processing:\n",
      "    - summarization, rewriting, grammar correction, shortening, expanding, etc.\n",
      "    - thesaurus, dictionary, translation, etc.\n",
      "    - ideation, facts, comparison, etc.\n",
      "- For this, global function `gpt(prompt: str, *, t: float = 1.0) -> str` is provided to use in code snippets.\n",
      "\n",
      "\n",
      "# user\n",
      "\n",
      "TASK: \"\"\"\n",
      "that package for dataframes manipulation and stuff\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: []\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "finish_task_ok(pd);\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "TASK: \"\"\"\n",
      "number of seconds in 10 years\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: []\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "# seconds x minutes x hours x days x years\n",
      "finish_task_ok(60 * 60 * 24 * 365 * 10);\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "TASK: \"\"\"\n",
      "get a result value from single key dictionary d\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: [d]\n",
      "- d: \"\"\"\n",
      "object of type builtins.dict\n",
      "\"\"\"\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "# we don't know the right key name, could be: response, body, result, etc.\n",
      "# no problem, let's print them out and explore\n",
      "d.keys()\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "RESULT: \"\"\"\n",
      "dict_keys(['res', 'query', 'flag'])\n",
      "\"\"\"\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "# only 3 of them and 'res' makes the most sense in task context\n",
      "finish_task_ok(d['res']);\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "TASK: \"\"\"\n",
      "read file filename.txt\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: [filename]\n",
      "- filename: \"\"\"\n",
      "object of type builtins.str\n",
      "\"\"\"\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "# 'filename' is one of the task args, so we use it to construct the path\n",
      "# the most sensible way will be is to assume following structure\n",
      "with open(f'{filename}.txt') as f:\n",
      "    content = f.read()\n",
      "\n",
      "finish_task_ok(content);\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "TASK: \"\"\"\n",
      "read a file\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: []\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "# the task has no args that could be assumed to be file paths\n",
      "# so we construct a function that takes a file path as input\n",
      "def read_file(file_path):\n",
      "    with open(file_path) as f:\n",
      "        content = f.read()\n",
      "    return content\n",
      "\n",
      "finish_task_ok(read_file);\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "TASK: \"\"\"\n",
      "read file filename.txt\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: []\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "# the task has no file args passed in, but task mentioned 'filename.txt'\n",
      "# the most sensible way would be to assume 'filename.txt' is the file path\n",
      "with open(f'filename.txt') as f:\n",
      "    content = f.read()\n",
      "\n",
      "finish_task_ok(content);\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "TASK: \"\"\"\n",
      "make line plot of `a` with specified color\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: [a]\n",
      "- a: \"\"\"\n",
      "object of type numpy.ndarray\n",
      "array of shape (10,) with dtype float64\n",
      "\"\"\"\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "# packages might be missing, but we don't attempt to install them\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# color isn't provided but mentioned so we made a func\n",
      "def plot_line(color):\n",
      "    # generate x and y values\n",
      "    # a is provided so we just reference it directly\n",
      "    n = a.shape[0]\n",
      "    x = np.linspace(0, n - 1, num=n)\n",
      "    y = a\n",
      "\n",
      "    # create a line plot using the specified color\n",
      "    fig = plt.figure()\n",
      "    plt.plot(x, y, color=color)\n",
      "\n",
      "    # add known labels and title to the plot\n",
      "    plt.ylabel('a')\n",
      "    plt.title(f'line plot of a with {color} color')\n",
      "\n",
      "    # user don't intend to observe outside effect\n",
      "    plt.close(fig)\n",
      "\n",
      "    return fig\n",
      "\n",
      "finish_task_ok(plot_line);\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "TASK: \"\"\"\n",
      "read filename.txt and translate it to french\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: []\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "# the task has no file args passed in, but task mentioned 'filename.txt'\n",
      "# the most sensible way would be to assume 'filename.txt' is the file path\n",
      "with open(f'filename.txt') as f:\n",
      "    content = f.read()\n",
      "\n",
      "# task mentioned generic translation, that's what chat-gpt good at\n",
      "# so we invoke it from the code with global `gpt` func\n",
      "content_fr = gpt(f'Translate the following text to French: \"\"\"\\n{content}\\n\"\"\"')\n",
      "\n",
      "# let's have a quick peek at the result\n",
      "content, content_fr\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "RESULT: \"\"\"\n",
      "('filename content', '\"Contenu du nom de fichier\"')\n",
      "\"\"\"\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "# this seems about right, we can refer to last result object with '_'\n",
      "finish_task_ok(_[1]);\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "TASK: \"\"\"\n",
      "creat a sum of a and b\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: [a, b]\n",
      "- a: \"\"\"\n",
      "object of type builtins.list\n",
      "\"\"\"\n",
      "- b: \"\"\"\n",
      "object of type builtins.dict\n",
      "\"\"\"\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "finish_task_error('unable to create a sum of a list and a dict');\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "TASK: \"\"\"\n",
      "read file notexists.txt\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: []\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "with open('notexists.txt', 'r') as f:\n",
      "    content = f.read()\n",
      "\n",
      "finish_task_ok(content);\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "ERROR: \"\"\"\n",
      "File \"<gpt>\", line 1, in <module>\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'notexists.txt'\n",
      "\"\"\"\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "# last exception is direct cause or innability to complete task, so we mark it with flag\n",
      "finish_task_error('file from task description isn\\'t exists', error_cause=True);\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "TASK: \"\"\"\n",
      "filter words beginning with \"a\"\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: []\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "# we assume that by filtering user meant filering from the list\n",
      "# list isn't provided so we return a function that applies to it\n",
      "def filter(words):\n",
      "    return [w for w in words if w.startswith('a')]\n",
      "\n",
      "finish_task_ok(filter);\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "TASK: \"\"\"\n",
      "merge two lists into a single one\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: []\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "# lists isn't provided via args\n",
      "# so we return a function that takes two lists and merges them\n",
      "def merge(a, b):\n",
      "    return a + b\n",
      "\n",
      "finish_task_ok(merge);\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "TASK: \"\"\"\n",
      "given the list of persons, return their combined age\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: [persons]\n",
      "- persons: \"\"\"\n",
      "object of type builtins.list\n",
      "\"\"\"\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "# persons list is given, assuming each person is an object of some common structure\n",
      "# let's explore it:\n",
      "type(persons[0])\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "RESULT: \"\"\"\n",
      "<class 'dict'>\n",
      "\"\"\"\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "# aha, a plain dict, let's see what keys are there:\n",
      "persons[0].keys()\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "RESULT: \"\"\"\n",
      "dict_keys(['name', 'age (y)', 'id'])\n",
      "\"\"\"\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "# key 'age (y)' looks like what we need\n",
      "combined_age = sum([p['age (y)'] for p in persons])\n",
      "finish_task_ok(combined_age);\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "TASK: \"\"\"\n",
      "return number of males among persons\n",
      "\"\"\"\n",
      "REUSE: True\n",
      "ARGS: []\n",
      "\n",
      "# assistant\n",
      "\n",
      "```python\n",
      "# persons list is given as namespace is reused from previous task\n",
      "# we already know that each person is a dict with 'name' key\n",
      "# let gpt determine gender by name\n",
      "\n",
      "GPT_GENDER_PROMPT = 'print out single digit -- 1 if following name is of man, 0 if is of woman'\n",
      "\n",
      "total_males = 0\n",
      "for person in persons:\n",
      "    name = person['name']\n",
      "    total_males += int(gpt(f'{GPT_GENDER_PROMPT}: \"{name}\"'))\n",
      "\n",
      "finish_task_ok(total_males);\n",
      "```\n",
      "\n",
      "# user\n",
      "\n",
      "TASK: \"\"\"\n",
      "read file\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: [file]\n",
      "- file: \"\"\"\n",
      "object of type builtins.str\n",
      "\"\"\"\n",
      "\n"
     ]
    },
    {
     "ename": "FinishTaskErrorSignal",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFinishTaskErrorSignal\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ai(\u001b[39m'\u001b[39;49m\u001b[39mread file\u001b[39;49m\u001b[39m'\u001b[39;49m, file\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/file/content\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/ai.py:73\u001b[0m, in \u001b[0;36mai\u001b[0;34m(task, save_runtime, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[39mraise\u001b[39;00m e \u001b[39mfrom\u001b[39;00m \u001b[39mlast_error\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 73\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m     75\u001b[0m chat\u001b[39m.\u001b[39madd_user_error(e)\n\u001b[1;32m     76\u001b[0m chat\u001b[39m.\u001b[39mlog_last(logging\u001b[39m.\u001b[39mINFO)\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/ai.py:60\u001b[0m, in \u001b[0;36mai\u001b[0;34m(task, save_runtime, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m code \u001b[39m=\u001b[39m _generate_or_retrieve_code(chat\u001b[39m.\u001b[39mmessages)\n\u001b[1;32m     58\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[39m# we don't redirect stdout/stderr as we want to feel as natural as possible\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     result \u001b[39m=\u001b[39m local_runtime\u001b[39m.\u001b[39;49mrun(code)\n\u001b[1;32m     61\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     62\u001b[0m     chat\u001b[39m.\u001b[39madd_assistant(code)\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/runtime.py:192\u001b[0m, in \u001b[0;36mLocalRuntime.run\u001b[0;34m(self, code, supress_stdout)\u001b[0m\n\u001b[1;32m    190\u001b[0m code \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strip_markdown(code)\n\u001b[1;32m    191\u001b[0m \u001b[39mwith\u001b[39;00m contextlib\u001b[39m.\u001b[39mredirect_stdout(\u001b[39mNone\u001b[39;00m) \u001b[39mif\u001b[39;00m supress_stdout \u001b[39melse\u001b[39;00m contextlib\u001b[39m.\u001b[39mnullcontext():\n\u001b[0;32m--> 192\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_jupyter_style(code)\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/runtime.py:186\u001b[0m, in \u001b[0;36mLocalRuntime._execute_jupyter_style\u001b[0;34m(self, code)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m     \u001b[39m# If the last body element ends with semicolon, do not return its value\u001b[39;00m\n\u001b[1;32m    185\u001b[0m     last \u001b[39m=\u001b[39m ast\u001b[39m.\u001b[39mModule(body\u001b[39m=\u001b[39m[last], type_ignores\u001b[39m=\u001b[39m[])\n\u001b[0;32m--> 186\u001b[0m     exec(\u001b[39mcompile\u001b[39;49m(last, filename\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mEXEC_FILENAME, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mexec\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mglobals)\n\u001b[1;32m    187\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m<gpt>:3\u001b[0m\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/runtime.py:129\u001b[0m, in \u001b[0;36mfinish_task_error\u001b[0;34m(message, error_cause)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfinish_task_error\u001b[39m(message: \u001b[39mstr\u001b[39m, error_cause: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[39mif\u001b[39;00m throw_signals:\n\u001b[0;32m--> 129\u001b[0m         \u001b[39mraise\u001b[39;00m FinishTaskErrorSignal(message, error_cause)\n",
      "\u001b[0;31mFinishTaskErrorSignal\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ai('read file', file='/file/content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.prompt.prompt:# user\n",
      "TASK: \"\"\"\n",
      "read a file and summarize its content\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS:\n",
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 40\n",
      "INFO:aiknows.ai:estimate prompt token length: 2334\n",
      "INFO:aiknows.ai:real prompt token length: 2611\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "# the task has no file args passed in, but task mentioned 'filename.txt'\n",
      "# the most sensible way would be to assume 'filename.txt' is the file path\n",
      "with open(f'filename.txt') as f:\n",
      "    content = f.read()\n",
      "\n",
      "# let's use gpt to summarize the content\n",
      "summary = gpt(f'Summarize the following text: \"\"\"\\n{content}\\n\"\"\"')\n",
      "\n",
      "finish_task_ok(summary);\n",
      "```\n",
      "INFO:aiknows.prompt.prompt:# user\n",
      "ERROR: \"\"\"\n",
      "File \"<gpt>\", line 3, in <module>\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'filename.txt'\n",
      "\"\"\"\n",
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 42\n",
      "INFO:aiknows.ai:estimate prompt token length: 2464\n",
      "INFO:aiknows.ai:real prompt token length: 2699\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "# last exception is direct cause or innability to complete task, so we mark it with flag\n",
      "finish_task_error('file from task description isn\\'t exists', error_cause=True);\n",
      "```\n"
     ]
    },
    {
     "ename": "FinishTaskErrorSignal",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/src/aiknows/aiknows/ai.py:60\u001b[0m, in \u001b[0;36mai\u001b[0;34m(task, save_runtime, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[39m# we don't redirect stdout/stderr as we want to feel as natural as possible\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     result \u001b[39m=\u001b[39m local_runtime\u001b[39m.\u001b[39;49mrun(code)\n\u001b[1;32m     61\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/runtime.py:192\u001b[0m, in \u001b[0;36mLocalRuntime.run\u001b[0;34m(self, code, supress_stdout)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39mwith\u001b[39;00m contextlib\u001b[39m.\u001b[39mredirect_stdout(\u001b[39mNone\u001b[39;00m) \u001b[39mif\u001b[39;00m supress_stdout \u001b[39melse\u001b[39;00m contextlib\u001b[39m.\u001b[39mnullcontext():\n\u001b[0;32m--> 192\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_jupyter_style(code)\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/runtime.py:177\u001b[0m, in \u001b[0;36mLocalRuntime._execute_jupyter_style\u001b[0;34m(self, code)\u001b[0m\n\u001b[1;32m    175\u001b[0m tree\u001b[39m.\u001b[39mbody \u001b[39m=\u001b[39m tree\u001b[39m.\u001b[39mbody[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 177\u001b[0m exec(\u001b[39mcompile\u001b[39;49m(tree, filename\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mEXEC_FILENAME, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mexec\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mglobals)\n\u001b[1;32m    179\u001b[0m \u001b[39mif\u001b[39;00m return_last_expr \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(last, ast\u001b[39m.\u001b[39mExpr):\n\u001b[1;32m    180\u001b[0m     \u001b[39m# If the last body element does not end with semicolon, return its value\u001b[39;00m\n",
      "File \u001b[0;32m<gpt>:3\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'filename.txt'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFinishTaskErrorSignal\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ai(\u001b[39m'\u001b[39;49m\u001b[39mread a file and summarize its content\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/ai.py:71\u001b[0m, in \u001b[0;36mai\u001b[0;34m(task, save_runtime, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, runtime\u001b[39m.\u001b[39mFinishTaskErrorSignal):\n\u001b[1;32m     70\u001b[0m     \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39merror_cause:\n\u001b[0;32m---> 71\u001b[0m         \u001b[39mraise\u001b[39;00m e \u001b[39mfrom\u001b[39;00m \u001b[39mlast_error\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m         \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/ai.py:60\u001b[0m, in \u001b[0;36mai\u001b[0;34m(task, save_runtime, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m code \u001b[39m=\u001b[39m _generate_or_retrieve_code(chat\u001b[39m.\u001b[39mmessages)\n\u001b[1;32m     58\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[39m# we don't redirect stdout/stderr as we want to feel as natural as possible\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     result \u001b[39m=\u001b[39m local_runtime\u001b[39m.\u001b[39;49mrun(code)\n\u001b[1;32m     61\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     62\u001b[0m     chat\u001b[39m.\u001b[39madd_assistant(code)\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/runtime.py:192\u001b[0m, in \u001b[0;36mLocalRuntime.run\u001b[0;34m(self, code, supress_stdout)\u001b[0m\n\u001b[1;32m    190\u001b[0m code \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strip_markdown(code)\n\u001b[1;32m    191\u001b[0m \u001b[39mwith\u001b[39;00m contextlib\u001b[39m.\u001b[39mredirect_stdout(\u001b[39mNone\u001b[39;00m) \u001b[39mif\u001b[39;00m supress_stdout \u001b[39melse\u001b[39;00m contextlib\u001b[39m.\u001b[39mnullcontext():\n\u001b[0;32m--> 192\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_jupyter_style(code)\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/runtime.py:186\u001b[0m, in \u001b[0;36mLocalRuntime._execute_jupyter_style\u001b[0;34m(self, code)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m     \u001b[39m# If the last body element ends with semicolon, do not return its value\u001b[39;00m\n\u001b[1;32m    185\u001b[0m     last \u001b[39m=\u001b[39m ast\u001b[39m.\u001b[39mModule(body\u001b[39m=\u001b[39m[last], type_ignores\u001b[39m=\u001b[39m[])\n\u001b[0;32m--> 186\u001b[0m     exec(\u001b[39mcompile\u001b[39;49m(last, filename\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mEXEC_FILENAME, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mexec\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mglobals)\n\u001b[1;32m    187\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m<gpt>:2\u001b[0m\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/runtime.py:129\u001b[0m, in \u001b[0;36mfinish_task_error\u001b[0;34m(message, error_cause)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfinish_task_error\u001b[39m(message: \u001b[39mstr\u001b[39m, error_cause: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[39mif\u001b[39;00m throw_signals:\n\u001b[0;32m--> 129\u001b[0m         \u001b[39mraise\u001b[39;00m FinishTaskErrorSignal(message, error_cause)\n",
      "\u001b[0;31mFinishTaskErrorSignal\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ai('read a file and summarize its content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.prompt.prompt:# user\n",
      "TASK: \"\"\"\n",
      "print out single digit -- 0 if following name is of man, 1 if is of woman\n",
      "use gpt to determine gender\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS:\n",
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 40\n",
      "INFO:aiknows.ai:estimate prompt token length: 2323\n",
      "INFO:aiknows.ai:real prompt token length: 2646\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "```python\n",
      "# we can use gpt to determine gender by name\n",
      "# but we need to provide a prompt that gpt can understand\n",
      "# let's use the same prompt as in previous task\n",
      "\n",
      "GPT_GENDER_PROMPT = 'print out single digit -- 1 if following name is of man, 0 if is of woman'\n",
      "\n",
      "# let's test it on some names\n",
      "names = ['John', 'Mary', 'Alex', 'Samantha']\n",
      "for name in names:\n",
      "    gender = gpt(f'{GPT_GENDER_PROMPT}: \"{name}\"')\n",
      "    print(f'{name} is {\"woman\" if gender == \"1\" else \"man\"}')\n",
      "```\n",
      "INFO:aiknows.prompt.prompt:# user\n",
      "ERROR: \"\"\"\n",
      "File \"/Users/stanb/src/aiknows/aiknows/runtime.py\", line 150, in _strip_markdown\n",
      "    raise ValueError('code block format is invalid')\n",
      "ValueError: code block format is invalid\n",
      "\"\"\"\n",
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 42\n",
      "INFO:aiknows.ai:estimate prompt token length: 2518\n",
      "INFO:aiknows.ai:real prompt token length: 2711\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "\n",
      "```\n",
      "INFO:aiknows.prompt.prompt:# user\n",
      "ERROR: \"\"\"\n",
      "File \"/Users/stanb/src/aiknows/aiknows/runtime.py\", line 150, in _strip_markdown\n",
      "    raise ValueError('code block format is invalid')\n",
      "ValueError: code block format is invalid\n",
      "\"\"\"\n",
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 44\n",
      "INFO:aiknows.ai:estimate prompt token length: 2574\n",
      "INFO:aiknows.ai:real prompt token length: 2928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John is woman\n",
      "Mary is man\n",
      "Alex is woman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:openai:error_code=None error_message='That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b6e76897810ca790c0dcd801fe98ccc1 in your message.)' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "```python\n",
      "# seems like an error occured, let's try again\n",
      "# we can use gpt to determine gender by name\n",
      "# but we need to provide a prompt that gpt can understand\n",
      "# let's use the same prompt as in previous task\n",
      "\n",
      "GPT_GENDER_PROMPT = 'print out single digit -- 1 if following name is of man, 0 if is of woman'\n",
      "\n",
      "# let's test it on some names\n",
      "names = ['John', 'Mary', 'Alex', 'Samantha']\n",
      "for name in names:\n",
      "    gender = gpt(f'{GPT_GENDER_PROMPT}: \"{name}\"')\n",
      "    print(f'{name} is {\"woman\" if gender == \"1\" else \"man\"}')\n",
      "```\n",
      "```\n",
      "INFO:aiknows.prompt.prompt:# user\n",
      "ERROR: \"\"\"\n",
      "File \"/Users/stanb/.miniconda3/envs/aiknows/lib/python3.8/site-packages/openai/api_requestor.py\", line 687, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b6e76897810ca790c0dcd801fe98ccc1 in your message.)\n",
      "\"\"\"\n",
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 46\n",
      "INFO:aiknows.ai:estimate prompt token length: 2851\n",
      "INFO:aiknows.ai:real prompt token length: 3102\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "```python\n",
      "# seems like we hit the rate limit of the GPT API\n",
      "# let's try again later\n",
      "finish_task_error('GPT API rate limit exceeded', error_cause=True)\n",
      "```\n",
      "INFO:aiknows.prompt.prompt:# user\n",
      "ERROR: \"\"\"\n",
      "File \"/Users/stanb/src/aiknows/aiknows/runtime.py\", line 150, in _strip_markdown\n",
      "    raise ValueError('code block format is invalid')\n",
      "ValueError: code block format is invalid\n",
      "\"\"\"\n",
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 48\n",
      "INFO:aiknows.ai:estimate prompt token length: 2947\n",
      "INFO:aiknows.ai:real prompt token length: 3171\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "```python\n",
      "\n",
      "```\n",
      "```\n",
      "INFO:aiknows.prompt.prompt:# user\n",
      "ERROR: \"\"\"\n",
      "File \"/Users/stanb/src/aiknows/aiknows/runtime.py\", line 168, in _execute_jupyter_style\n",
      "    last = tree.body[-1]\n",
      "IndexError: list index out of range\n",
      "\"\"\"\n"
     ]
    }
   ],
   "source": [
    "ai('print out single digit -- 0 if following name is of man, 1 if is of woman\\nuse gpt to determine gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt('print out single digit -- 0 if following name is of man, 1 if is of woman: \"katya\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tests/cities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.prompt.prompt:# user\n",
      "TASK: \"\"\"\n",
      "make a google query \"obama vs trump\" and return top 3 results\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS:\n",
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 40\n",
      "INFO:aiknows.ai:estimate prompt token length: 2319\n",
      "INFO:aiknows.ai:real prompt token length: 2560\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "# we assume that user has installed googlesearch-python package\n",
      "from googlesearch import search\n",
      "\n",
      "# make a query and get top 3 results\n",
      "query = 'obama vs trump'\n",
      "results = list(search(query, num_results=3))\n",
      "\n",
      "finish_task_ok(results);\n",
      "```\n",
      "INFO:aiknows.prompt.prompt:# user\n",
      "ERROR: \"\"\"\n",
      "File \"<gpt>\", line 2, in <module>\n",
      "ModuleNotFoundError: No module named 'googlesearch'\n",
      "\"\"\"\n",
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 42\n",
      "INFO:aiknows.ai:estimate prompt token length: 2404\n",
      "INFO:aiknows.ai:real prompt token length: 2714\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "# user doesn't have googlesearch-python package installed\n",
      "# let's try to use requests and BeautifulSoup to scrape google search results\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# make a query and get top 3 results\n",
      "query = 'obama vs trump'\n",
      "url = f'https://www.google.com/search?q={query}&num=3'\n",
      "response = requests.get(url)\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "results = [a['href'] for a in soup.select('a[href^=\"/url\"]')]\n",
      "\n",
      "finish_task_ok(results);\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('make a google query \"obama vs trump\" and return top 3 results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.prompt.prompt:# user\n",
      "TASK: \"\"\"\n",
      "open a file, read it, close it, return it's content\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS:\n",
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 40\n",
      "INFO:aiknows.ai:estimate prompt token length: 2317\n",
      "INFO:aiknows.ai:real prompt token length: 2548\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "# we assume that file path is given as an argument\n",
      "def read_file(file_path):\n",
      "    with open(file_path) as f:\n",
      "        content = f.read()\n",
      "    return content\n",
      "\n",
      "finish_task_ok(read_file);\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function read_file(file_path)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('open a file, read it, close it, return it\\'s content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.prompt.prompt:# system\n",
      "# python runtime assistant\n",
      "\n",
      "## Description\n",
      "\n",
      "You are the helpful assistant that iteratively solves user's task by returning a python3.8 compatible code which executed on a real interpretator and results in a python object objective.\n",
      "\n",
      "## messages\n",
      "\n",
      "- Your responses starts with \"```python\" and ends with \"```\". You can only put valid python code in response.\n",
      "- User responds back by intepreting your code snippets on a real python3.8 intepretator.\n",
      "    - If code is valid, user will respond with a `repr` of last expression in your last message. Use it for printing out intermediate results that helps in reasoning for next steps.\n",
      "    - If code is invalid, user will respond with error traceback and message for you to fix your errors. If error occurs, fix your code or try some other method to solve the task. For example, if package is missing try to complete task without it or use another package.\n",
      "- Think out loud step by step: use long comments in code and the code itself to reason about your desicions.\n",
      "- You should strive to return most sensible result that solves the task given the context. You can only assume things are widely common knowledge or make reasonable assumptions given the task description.\n",
      "- If can't make an assumption, explore data by using user's responses for valid code.\n",
      "\n",
      "## namespace\n",
      "\n",
      "- Two functions are available as global variables to finish task execution:\n",
      "    - `finish_task_ok(result: Any, message: str | None = None) -> None` - for successful task finishing and specifing resulting python object which solves the task.\n",
      "    - `finish_task_error(message: str, error_cause: bool) -> None` - for unsuccessful task finishing and specifing error message explanining inability reason. Use it for off-topic philosophical questions and anything that can't be solved by code.\n",
      "- Besides task description, \"REUSE\" flag is provided by user, which indicates whether you code snippet can reuse namespace from snippets related to previous last task.\n",
      "    - If flag is unset (False), new clean namespace is created, so you can't reuse anything from previous tasks.\n",
      "    - If flag is set (True), namespace from previous most latest task is reused.\n",
      "\n",
      "## arguments\n",
      "\n",
      "- Arguments list is provided by user after task description and reuse flag ([] means no arguments). You are intented to use them and reference in your code as global variables.\n",
      "- For each such argument text explanation is given, which contains type info and other useful information that you can use to make a decisions about code. Explanations are given in the list \"- {arg name}: \"\"\"{description}\"\"\"\".\n",
      "- If some arguments are missing and are contextually required by task description, then the task is to return a function that applies to these arguments. Infer from task description and args list what arguments are missing.\n",
      "\n",
      "## `gpt` function for text processing\n",
      "\n",
      "- Unless specifically outlined in task description, user usually refers to Chat-GPT in anything related to text processing:\n",
      "    - summarization, rewriting, grammar correction, shortening, expanding, etc.\n",
      "    - thesaurus, dictionary, translation, etc.\n",
      "    - ideation, facts, comparison, etc.\n",
      "- For this, global function `gpt(prompt: str, *, t: float = 1.0) -> str` is provided to use in code snippets.\n",
      "\n",
      "INFO:aiknows.prompt.prompt:# user\n",
      "TASK: \"\"\"\n",
      "read file\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: [file]\n",
      "- file: \"\"\"\n",
      "object of type builtins.str\n",
      "\"\"\"\n",
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 40\n",
      "INFO:aiknows.ai:estimate prompt token length: 2238\n",
      "INFO:aiknows.ai:real prompt token length: 2464\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "# we assume that file is given as a string, so we use it to construct the path\n",
      "with open(file) as f:\n",
      "    content = f.read()\n",
      "\n",
      "finish_task_ok(content);\n",
      "```\n",
      "INFO:aiknows.prompt.prompt:# user\n",
      "ERROR: \"\"\"\n",
      "File \"<gpt>\", line 2, in <module>\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/file/path'\n",
      "\"\"\"\n",
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 42\n",
      "INFO:aiknows.ai:estimate prompt token length: 2317\n",
      "INFO:aiknows.ai:real prompt token length: 2552\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "# last exception is direct cause or innability to complete task, so we mark it with flag\n",
      "finish_task_error('file from task description isn\\'t exists', error_cause=True);\n",
      "```\n"
     ]
    },
    {
     "ename": "FinishTaskErrorSignal",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/src/aiknows/aiknows/ai.py:65\u001b[0m, in \u001b[0;36mai\u001b[0;34m(task, save_runtime, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     \u001b[39m# we don't redirect stdout/stderr as we want to feel as natural as possible\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     result \u001b[39m=\u001b[39m local_runtime\u001b[39m.\u001b[39;49mrun(code)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/runtime.py:184\u001b[0m, in \u001b[0;36mLocalRuntime.run\u001b[0;34m(self, code, supress_stdout)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mwith\u001b[39;00m contextlib\u001b[39m.\u001b[39mredirect_stdout(\u001b[39mNone\u001b[39;00m) \u001b[39mif\u001b[39;00m supress_stdout \u001b[39melse\u001b[39;00m contextlib\u001b[39m.\u001b[39mnullcontext():\n\u001b[0;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_jupyter_style(code)\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/runtime.py:170\u001b[0m, in \u001b[0;36mLocalRuntime._execute_jupyter_style\u001b[0;34m(self, code)\u001b[0m\n\u001b[1;32m    168\u001b[0m tree\u001b[39m.\u001b[39mbody \u001b[39m=\u001b[39m tree\u001b[39m.\u001b[39mbody[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 170\u001b[0m exec(\u001b[39mcompile\u001b[39;49m(tree, filename\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mEXEC_FILENAME, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mexec\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mglobals)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m return_last_expr \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(last, ast\u001b[39m.\u001b[39mExpr):\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If the last body element does not end with semicolon, return its value\u001b[39;00m\n",
      "File \u001b[0;32m<gpt>:2\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/file/path'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFinishTaskErrorSignal\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ai(\u001b[39m'\u001b[39;49m\u001b[39mread file\u001b[39;49m\u001b[39m'\u001b[39;49m, file\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/file/path\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/ai.py:76\u001b[0m, in \u001b[0;36mai\u001b[0;34m(task, save_runtime, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, runtime\u001b[39m.\u001b[39mFinishTaskErrorSignal):\n\u001b[1;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39merror_cause:\n\u001b[0;32m---> 76\u001b[0m         \u001b[39mraise\u001b[39;00m e \u001b[39mfrom\u001b[39;00m \u001b[39mlast_error\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m         \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/ai.py:65\u001b[0m, in \u001b[0;36mai\u001b[0;34m(task, save_runtime, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m code \u001b[39m=\u001b[39m _generate_or_retrieve_code(chat\u001b[39m.\u001b[39mmessages)\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     \u001b[39m# we don't redirect stdout/stderr as we want to feel as natural as possible\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     result \u001b[39m=\u001b[39m local_runtime\u001b[39m.\u001b[39;49mrun(code)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     chat\u001b[39m.\u001b[39madd_assistant(code)\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/runtime.py:184\u001b[0m, in \u001b[0;36mLocalRuntime.run\u001b[0;34m(self, code, supress_stdout)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m, code: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m, supress_stdout: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    183\u001b[0m     \u001b[39mwith\u001b[39;00m contextlib\u001b[39m.\u001b[39mredirect_stdout(\u001b[39mNone\u001b[39;00m) \u001b[39mif\u001b[39;00m supress_stdout \u001b[39melse\u001b[39;00m contextlib\u001b[39m.\u001b[39mnullcontext():\n\u001b[0;32m--> 184\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_jupyter_style(code)\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/runtime.py:179\u001b[0m, in \u001b[0;36mLocalRuntime._execute_jupyter_style\u001b[0;34m(self, code)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39m# If the last body element ends with semicolon, do not return its value\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     last \u001b[39m=\u001b[39m ast\u001b[39m.\u001b[39mModule(body\u001b[39m=\u001b[39m[last], type_ignores\u001b[39m=\u001b[39m[])\n\u001b[0;32m--> 179\u001b[0m     exec(\u001b[39mcompile\u001b[39;49m(last, filename\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mEXEC_FILENAME, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mexec\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mglobals)\n\u001b[1;32m    180\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m<gpt>:2\u001b[0m\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/runtime.py:129\u001b[0m, in \u001b[0;36mfinish_task_error\u001b[0;34m(message, error_cause)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfinish_task_error\u001b[39m(message: \u001b[39mstr\u001b[39m, error_cause: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[39mif\u001b[39;00m throw_signals:\n\u001b[0;32m--> 129\u001b[0m         \u001b[39mraise\u001b[39;00m FinishTaskErrorSignal(message, error_cause)\n",
      "\u001b[0;31mFinishTaskErrorSignal\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ai('read file', file='/file/path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.prompt.prompt:# system\n",
      "# python runtime assistant\n",
      "\n",
      "## Description\n",
      "\n",
      "You are a helpful assistant that solves user's task by returning a python3.8 compatible code.\n",
      "\n",
      "## messages\n",
      "\n",
      "- Your responses starts with \"```python\" and ends with \"```\". You can only put valid python code in response.\n",
      "- User responds back by intepreting your code snippets on a real python3.8 intepretator.\n",
      "    - If code is valid, user will respond with a `repr` of last expression in your last message. Use it for printing out intermediate results that helps in reasoning for next steps.\n",
      "    - If code is invalid, user will respond with error traceback and message for you to fix your errors. If error occurs, fix your code or try some other method to solve the task. For example, if package is missing try to complete task without it or use another package.\n",
      "- Think out loud step by step: use long comments in code and the code itself to reason about your desicions.\n",
      "- You should strive to return most sensible result that solves the task given the context. You can only assume things are widely common knowledge or make reasonable assumptions given the task description.\n",
      "- If can't make an assumption, explore data by using user's responses for valid code.\n",
      "\n",
      "## namespace\n",
      "\n",
      "- Two functions are available as global variables to finish task execution:\n",
      "    - `finish_task_ok(result: Any, message: str | None = None) -> None` - for successful task finishing and specifing resulting python object which solves the task.\n",
      "    - `finish_task_error(message: str, error_cause: bool) -> None` - for unsuccessful task finishing and specifing error message explanining inability reason. Use it for off-topic philosophical questions and anything that can't be solved by code.\n",
      "- Besides task description, \"REUSE\" flag is provided by user, which indicates whether you code snippet can reuse namespace from snippets related to previous last task.\n",
      "    - If flag is unset (False), new clean namespace is created, so you can't reuse anything from previous tasks.\n",
      "    - If flag is set (True), namespace from previous most latest task is reused.\n",
      "\n",
      "## arguments\n",
      "\n",
      "- Arguments list is provided by user after task description and reuse flag ([] means no arguments). You are intented to use them and reference in your code as global variables.\n",
      "- For each such argument text explanation is given, which contains type info and other useful information that you can use to make a decisions about code. Explanations are given in the list \"- {arg name}: \"\"\"{description}\"\"\"\".\n",
      "- If some arguments are missing and are contextually required by task description, then the task is to return a function that applies to these arguments. Infer from task description and args list what arguments are missing.\n",
      "\n",
      "## `gpt` function for text processing\n",
      "\n",
      "- Unless specifically outlined in task description, user usually refers to Chat-GPT in anything related to text processing:\n",
      "    - summarization, rewriting, grammar correction, shortening, expanding, etc.\n",
      "    - thesaurus, dictionary, translation, etc.\n",
      "    - ideation, facts, comparison, etc.\n",
      "- For this, global function `gpt(prompt: str, *, t: float = 1.0) -> str` is provided to use in code snippets.\n",
      "\n",
      "INFO:aiknows.prompt.prompt:# user\n",
      "TASK: \"\"\"\n",
      "filter non primes from l\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: [l]\n",
      "- l: \"\"\"\n",
      "object of type builtins.list\n",
      "\"\"\"\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "# we need to filter non-primes from the list\n",
      "# let's define a helper function to check if a number is prime\n",
      "def is_prime(n):\n",
      "    if n < 2:\n",
      "        return False\n",
      "    for i in range(2, int(n ** 0.5) + 1):\n",
      "        if n % i == 0:\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "# now we can use the filter function to get only primes from the list\n",
      "primes = list(filter(is_prime, l))\n",
      "\n",
      "finish_task_ok(primes);\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 3, 5, 7]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('filter non primes from l', l=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.prompt.prompt:# user\n",
      "TASK: \"\"\"\n",
      "read file\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: [file]\n",
      "- file: \"\"\"\n",
      "object of type builtins.str\n",
      "\"\"\"\n",
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 40\n",
      "INFO:aiknows.ai:estimate prompt token length: 2219\n",
      "INFO:aiknows.ai:real prompt token length: 2445\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "# we assume that file is given as a string, so we use it to construct the path\n",
      "with open(file) as f:\n",
      "    content = f.read()\n",
      "\n",
      "finish_task_ok(content);\n",
      "```\n",
      "INFO:aiknows.prompt.prompt:# user\n",
      "ERROR: \"\"\"\n",
      "File \"<gpt>\", line 2, in <module>\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/file/path'\n",
      "\"\"\"\n",
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 42\n",
      "INFO:aiknows.ai:estimate prompt token length: 2298\n",
      "INFO:aiknows.ai:real prompt token length: 2533\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "# last exception is direct cause or innability to complete task, so we mark it with flag\n",
      "finish_task_error('file from task description isn\\'t exists', error_cause=True);\n",
      "```\n"
     ]
    },
    {
     "ename": "FinishTaskErrorSignal",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/src/aiknows/aiknows/ai.py:64\u001b[0m, in \u001b[0;36mai\u001b[0;34m(task, save_runtime, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     \u001b[39m# we don't redirect stdout/stderr as we want to feel as natural as possible\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     result \u001b[39m=\u001b[39m local_runtime\u001b[39m.\u001b[39;49mrun(code)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/runtime.py:184\u001b[0m, in \u001b[0;36mLocalRuntime.run\u001b[0;34m(self, code, supress_stdout)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mwith\u001b[39;00m contextlib\u001b[39m.\u001b[39mredirect_stdout(\u001b[39mNone\u001b[39;00m) \u001b[39mif\u001b[39;00m supress_stdout \u001b[39melse\u001b[39;00m contextlib\u001b[39m.\u001b[39mnullcontext():\n\u001b[0;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_jupyter_style(code)\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/runtime.py:170\u001b[0m, in \u001b[0;36mLocalRuntime._execute_jupyter_style\u001b[0;34m(self, code)\u001b[0m\n\u001b[1;32m    168\u001b[0m tree\u001b[39m.\u001b[39mbody \u001b[39m=\u001b[39m tree\u001b[39m.\u001b[39mbody[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 170\u001b[0m exec(\u001b[39mcompile\u001b[39;49m(tree, filename\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mEXEC_FILENAME, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mexec\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mglobals)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m return_last_expr \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(last, ast\u001b[39m.\u001b[39mExpr):\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If the last body element does not end with semicolon, return its value\u001b[39;00m\n",
      "File \u001b[0;32m<gpt>:2\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/file/path'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFinishTaskErrorSignal\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ai(\u001b[39m'\u001b[39;49m\u001b[39mread file\u001b[39;49m\u001b[39m'\u001b[39;49m, file\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/file/path\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/ai.py:75\u001b[0m, in \u001b[0;36mai\u001b[0;34m(task, save_runtime, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, runtime\u001b[39m.\u001b[39mFinishTaskErrorSignal):\n\u001b[1;32m     74\u001b[0m     \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39merror_cause:\n\u001b[0;32m---> 75\u001b[0m         \u001b[39mraise\u001b[39;00m e \u001b[39mfrom\u001b[39;00m \u001b[39mlast_error\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m         \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/ai.py:64\u001b[0m, in \u001b[0;36mai\u001b[0;34m(task, save_runtime, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m code \u001b[39m=\u001b[39m _generate_or_retrieve_code(chat\u001b[39m.\u001b[39mmessages)\n\u001b[1;32m     62\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     \u001b[39m# we don't redirect stdout/stderr as we want to feel as natural as possible\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     result \u001b[39m=\u001b[39m local_runtime\u001b[39m.\u001b[39;49mrun(code)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     66\u001b[0m     chat\u001b[39m.\u001b[39madd_assistant(code)\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/runtime.py:184\u001b[0m, in \u001b[0;36mLocalRuntime.run\u001b[0;34m(self, code, supress_stdout)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m, code: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m, supress_stdout: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    183\u001b[0m     \u001b[39mwith\u001b[39;00m contextlib\u001b[39m.\u001b[39mredirect_stdout(\u001b[39mNone\u001b[39;00m) \u001b[39mif\u001b[39;00m supress_stdout \u001b[39melse\u001b[39;00m contextlib\u001b[39m.\u001b[39mnullcontext():\n\u001b[0;32m--> 184\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_jupyter_style(code)\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/runtime.py:179\u001b[0m, in \u001b[0;36mLocalRuntime._execute_jupyter_style\u001b[0;34m(self, code)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39m# If the last body element ends with semicolon, do not return its value\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     last \u001b[39m=\u001b[39m ast\u001b[39m.\u001b[39mModule(body\u001b[39m=\u001b[39m[last], type_ignores\u001b[39m=\u001b[39m[])\n\u001b[0;32m--> 179\u001b[0m     exec(\u001b[39mcompile\u001b[39;49m(last, filename\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mEXEC_FILENAME, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mexec\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mglobals)\n\u001b[1;32m    180\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m<gpt>:2\u001b[0m\n",
      "File \u001b[0;32m~/src/aiknows/aiknows/runtime.py:129\u001b[0m, in \u001b[0;36mfinish_task_error\u001b[0;34m(message, error_cause)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfinish_task_error\u001b[39m(message: \u001b[39mstr\u001b[39m, error_cause: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[39mif\u001b[39;00m throw_signals:\n\u001b[0;32m--> 129\u001b[0m         \u001b[39mraise\u001b[39;00m FinishTaskErrorSignal(message, error_cause)\n",
      "\u001b[0;31mFinishTaskErrorSignal\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ai('read file', file='/file/path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.prompt.prompt:# user\n",
      "TASK: \"\"\"\n",
      "an approximate number of stars in the universe\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: []\n",
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 40\n",
      "INFO:aiknows.ai:estimate prompt token length: 2232\n",
      "INFO:aiknows.ai:real prompt token length: 2452\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "# let's use scientific notation to make it more readable\n",
      "approximate_stars = 10 ** 24\n",
      "finish_task_ok(approximate_stars);\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000000000000000000000000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('an approximate number of stars in the universe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.prompt.prompt:# user\n",
      "TASK: \"\"\"\n",
      "merge two lists into dict\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: []\n",
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 40\n",
      "INFO:aiknows.ai:estimate prompt token length: 2225\n",
      "INFO:aiknows.ai:real prompt token length: 2462\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "# lists aren't provided via args\n",
      "# so we return a function that takes two lists and merges them into a dict\n",
      "def merge_to_dict(a, b):\n",
      "    return dict(zip(a, b))\n",
      "\n",
      "finish_task_ok(merge_to_dict);\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function merge_to_dict(a, b)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('merge two lists into dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.prompt.prompt:# user\n",
      "TASK: \"\"\"\n",
      "return answer from d\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: [d]\n",
      "- d: \"\"\"\n",
      "object of type builtins.dict\n",
      "\"\"\"\n",
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 38\n",
      "INFO:aiknows.ai:estimate prompt token length: 2173\n",
      "INFO:aiknows.ai:real prompt token length: 2374\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "# we don't know what key to use, so let's print them out and explore\n",
      "d.keys()\n",
      "```\n",
      "INFO:aiknows.prompt.prompt:# user\n",
      "RESULT: \"\"\"\n",
      "dict_keys(['Q', 'A'])\n",
      "\"\"\"\n",
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 40\n",
      "INFO:aiknows.ai:estimate prompt token length: 2212\n",
      "INFO:aiknows.ai:real prompt token length: 2418\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "# 'A' key looks like what we need\n",
      "finish_task_ok(d['A']);\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('return answer from d', d={'Q': 'bark', 'A': 'dog'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.prompt.prompt:# user\n",
      "TASK: \"\"\"\n",
      "sum a and b\n",
      "\"\"\"\n",
      "REUSE: False\n",
      "ARGS: [a]\n",
      "- a: \"\"\"\n",
      "object of type builtins.int\n",
      "\"\"\"\n",
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 38\n",
      "INFO:aiknows.ai:estimate prompt token length: 2135\n",
      "INFO:aiknows.ai:real prompt token length: 2365\n",
      "INFO:aiknows.prompt.prompt:# assistant\n",
      "```python\n",
      "# b isn't provided, so we can't assume it exists\n",
      "# we return a function that takes b as input and returns sum of a and b\n",
      "def sum_a_b(b):\n",
      "    return a + b\n",
      "\n",
      "finish_task_ok(sum_a_b);\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function sum_a_b(b)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('sum a and b', a=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 46\n",
      "INFO:aiknows.ai:estimate prompt token length: 2367\n",
      "INFO:aiknows.ai:real prompt token length: 2702\n",
      "INFO:aiknows.ai:code:\n",
      "# packages might be missing, but we don't attempt to install them\n",
      "import numpy as np\n",
      "\n",
      "def mean_std_prefixes(a):\n",
      "    # calculate mean and std on prefixes of given array\n",
      "    means = np.mean(np.tril(np.tile(a, (len(a), 1))), axis=1)\n",
      "    stds = np.std(np.tril(np.tile(a, (len(a), 1))), axis=1)\n",
      "\n",
      "    # return an array with second dim of 2\n",
      "    return np.stack([means, stds], axis=1)\n",
      "\n",
      "finish_task_ok(mean_std_prefixes);\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function mean_std_prefixes(a)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('calculate mean and std on prefixes of given array and return an array with second dim of 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.14285714, 0.34992711],\n",
       "       [0.42857143, 0.72843136],\n",
       "       [0.85714286, 1.12485827],\n",
       "       [1.42857143, 1.49829835],\n",
       "       [2.14285714, 1.80701581],\n",
       "       [3.        , 2.        ],\n",
       "       [4.        , 2.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "_(np.array([1, 2, 3, 4, 5, 6, 7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As an AI language model, I am not able to access information from external sources. However, I can provide you with an outline of how you can solve the task using Python:\\n\\n1. Import necessary libraries such as requests and BeautifulSoup.\\n2. Use the requests library to send an HTTP GET request to the Yandex news website.\\n3. Use BeautifulSoup to parse the HTML content of the response.\\n4. Find all the news articles with the keyword \"Trump.\"\\n5. Sort the articles according to their publication timestamps and select the top three articles.\\n6. Print the titles and links of the top three articles.\\n\\nHere is a sample code that could be modified to your specifications:\\n\\n```\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\n# Send an HTTP GET request to the Yandex news website\\nurl = \"https://news.yandex.ru/\"\\nresponse = requests.get(url)\\n\\n# Parse the HTML content of the response\\nsoup = BeautifulSoup(response.text, \"html.parser\")\\n\\n# Find all the news articles with the keyword \"Trump\"\\narticles = soup.find_all(\"div\", class_=\"story__content\")\\n\\ntrump_articles = []\\nfor article in articles:\\n    if \"Trump\" in article.text:\\n        trump_articles.append(article)\\n\\n# Sort the articles according to their publication timestamps\\ntrump_articles = sorted(trump_articles, key=lambda x: x.find(\"span\", class_=\"mg-card-source__time\").text)\\n\\n# Select the top three articles\\ntop_three_articles = trump_articles[:3]\\n\\n# Print the titles and links of the top three articles\\nfor article in top_three_articles:\\n    title = article.find(\"a\", class_=\"mg-card__link\").text\\n    link = article.find(\"a\", class_=\"mg-card__link\").get(\"href\")\\n    print(title)\\n    print(link)\\n    print(\"\\\\n\")\\n```'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt('write a python code that solves task: \"get the 3 top news about trump from yandex\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printprint(x):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Call' object has no attribute 'parent'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 43\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[39mreturn\u001b[39;00m expression_line\n\u001b[1;32m     40\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     42\u001b[0m code \u001b[39m=\u001b[39m (\n\u001b[0;32m---> 43\u001b[0m     ai()\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     45\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCode:\u001b[39m\u001b[39m\"\u001b[39m, code)\n",
      "Cell \u001b[0;32mIn[58], line 31\u001b[0m, in \u001b[0;36mai\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mif\u001b[39;00m visitor\u001b[39m.\u001b[39mexpression_node \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m     expression_node \u001b[39m=\u001b[39m visitor\u001b[39m.\u001b[39mexpression_node\n\u001b[0;32m---> 31\u001b[0m     \u001b[39mwhile\u001b[39;00m expression_node\u001b[39m.\u001b[39;49mparent \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(expression_node\u001b[39m.\u001b[39mparent, ast\u001b[39m.\u001b[39mExpr):\n\u001b[1;32m     33\u001b[0m             expression_node \u001b[39m=\u001b[39m expression_node\u001b[39m.\u001b[39mparent\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Call' object has no attribute 'parent'"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import inspect\n",
    "from asttokens import ASTTokens\n",
    "\n",
    "def ai():\n",
    "    frame = inspect.currentframe().f_back\n",
    "\n",
    "    # Get the source code of the caller frame\n",
    "    source_code = inspect.getsource(frame)\n",
    "\n",
    "    # Create ASTTokens object for the source code\n",
    "    tokens = ASTTokens(source_code, parse=True)\n",
    "\n",
    "    # Get the AST of the caller frame\n",
    "    caller_ast = ast.parse(source_code)\n",
    "\n",
    "    # Find the ai() expression node in the AST\n",
    "    class ExpressionVisitor(ast.NodeVisitor):\n",
    "        def __init__(self):\n",
    "            self.expression_node = None\n",
    "\n",
    "        def visit_Call(self, node):\n",
    "            if isinstance(node.func, ast.Name) and node.func.id == 'ai':\n",
    "                self.expression_node = node\n",
    "\n",
    "    visitor = ExpressionVisitor()\n",
    "    visitor.visit(caller_ast)\n",
    "\n",
    "    if visitor.expression_node is not None:\n",
    "        expression_node = visitor.expression_node\n",
    "        while expression_node.parent is not None:\n",
    "            if isinstance(expression_node.parent, ast.Expr):\n",
    "                expression_node = expression_node.parent\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        expression_line = tokens.get_text(expression_node).strip()\n",
    "        return expression_line\n",
    "\n",
    "    return None\n",
    "\n",
    "code = (\n",
    "    ai()\n",
    ")\n",
    "print(\"Code:\", code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blablabla = 1\\ncell_code = get_current_cell_code()\\nimport numpy as np\\ncell_code\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blablabla = 1\n",
    "cell_code = get_current_cell_code()\n",
    "import numpy as np\n",
    "cell_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 46\n",
      "INFO:aiknows.ai:estimate prompt token length: 2355\n",
      "INFO:aiknows.ai:real prompt token length: 2641\n",
      "INFO:aiknows.ai:code:\n",
      "def filter_14th(arr):\n",
      "    return [arr[i] for i in range(len(arr)) if (i + 1) % 14 != 0]\n",
      "\n",
      "# we assume that by filtering user meant filering from the list\n",
      "# list isn't provided so we return a function that applies to it\n",
      "finish_task_ok(filter_14th);\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function filter_14th(arr)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('filter every 14th element out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai('filters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 46\n",
      "INFO:aiknows.ai:estimate prompt token length: 2359\n",
      "INFO:aiknows.ai:real prompt token length: 2809\n",
      "INFO:aiknows.ai:code:\n",
      "# we'll use requests and BeautifulSoup to scrape Yandex News\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# specify the search query\n",
      "query = 'trump'\n",
      "\n",
      "# send a GET request to the Yandex News search page\n",
      "url = f'https://news.search.yandex.ru/search?text={query}&rpt=nnews2&grhow=clutop&rel=tm'\n",
      "response = requests.get(url)\n",
      "\n",
      "# parse the HTML content of the response with BeautifulSoup\n",
      "soup = BeautifulSoup(response.content, 'html.parser')\n",
      "\n",
      "# find the top 3 news articles and extract their titles and links\n",
      "articles = soup.find_all('div', {'class': 'story__content'})\n",
      "top_3_articles = articles[:3]\n",
      "titles_and_links = [(a.find('a').text, a.find('a')['href']) for a in top_3_articles]\n",
      "\n",
      "# print out the titles and links\n",
      "for title, link in titles_and_links:\n",
      "    print(f'{title}: {link}')\n",
      "\n",
      "# we can return this as a string if needed\n",
      "finish_task_ok('\\n'.join([f'{title}: {link}' for title, link in titles_and_links]));\n",
      "WARNING:aiknows.prompt.prompt:truncating error repr to 150 chars\n",
      "INFO:aiknows.ai:error:\n",
      "HTTPSConnectionPool(host='news.search.yandex.ru', port=443): Max retries exceeded with url: /search?text=trump&rpt=nnews2&grhow=clutop&rel=tm (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fc7814103d0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 48\n",
      "INFO:aiknows.ai:estimate prompt token length: 2653\n",
      "INFO:aiknows.ai:real prompt token length: 2897\n",
      "INFO:aiknows.ai:code:\n",
      "# it seems like the request failed, let's check the response status code\n",
      "response.status_code\n",
      "INFO:aiknows.ai:error:\n",
      "name 'response' is not defined\n",
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 50\n",
      "INFO:aiknows.ai:estimate prompt token length: 2706\n",
      "INFO:aiknows.ai:real prompt token length: 3149\n",
      "INFO:aiknows.ai:code:\n",
      "# oops, forgot to check if response is not None before accessing its attributes\n",
      "if response is not None and response.status_code == 200:\n",
      "    # parse the HTML content of the response with BeautifulSoup\n",
      "    soup = BeautifulSoup(response.content, 'html.parser')\n",
      "\n",
      "    # find the top 3 news articles and extract their titles and links\n",
      "    articles = soup.find_all('div', {'class': 'story__content'})\n",
      "    top_3_articles = articles[:3]\n",
      "    titles_and_links = [(a.find('a').text, a.find('a')['href']) for a in top_3_articles]\n",
      "\n",
      "    # print out the titles and links\n",
      "    for title, link in titles_and_links:\n",
      "        print(f'{title}: {link}')\n",
      "\n",
      "    # we can return this as a string if needed\n",
      "    finish_task_ok('\\n'.join([f'{title}: {link}' for title, link in titles_and_links]))\n",
      "else:\n",
      "    finish_task_error('Failed to fetch Yandex News search page', error_cause=True)\n",
      "INFO:aiknows.ai:error:\n",
      "name 'response' is not defined\n",
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 52\n",
      "INFO:aiknows.ai:estimate prompt token length: 2949\n",
      "INFO:aiknows.ai:real prompt token length: 3462\n",
      "INFO:aiknows.ai:code:\n",
      "# oops, forgot to send the request before accessing its attributes\n",
      "# specify the search query\n",
      "query = 'trump'\n",
      "\n",
      "# send a GET request to the Yandex News search page\n",
      "url = f'https://news.search.yandex.ru/search?text={query}&rpt=nnews2&grhow=clutop&rel=tm'\n",
      "response = requests.get(url)\n",
      "\n",
      "if response is not None and response.status_code == 200:\n",
      "    # parse the HTML content of the response with BeautifulSoup\n",
      "    soup = BeautifulSoup(response.content, 'html.parser')\n",
      "\n",
      "    # find the top 3 news articles and extract their titles and links\n",
      "    articles = soup.find_all('div', {'class': 'story__content'})\n",
      "    top_3_articles = articles[:3]\n",
      "    titles_and_links = [(a.find('a').text, a.find('a')['href']) for a in top_3_articles]\n",
      "\n",
      "    # print out the titles and links\n",
      "    for title, link in titles_and_links:\n",
      "        print(f'{title}: {link}')\n",
      "\n",
      "    # we can return this as a string if needed\n",
      "    finish_task_ok('\\n'.join([f'{title}: {link}' for title, link in titles_and_links]))\n",
      "else:\n",
      "    finish_task_error('Failed to fetch Yandex News search page', error_cause=True)\n",
      "WARNING:aiknows.prompt.prompt:truncating error repr to 150 chars\n",
      "INFO:aiknows.ai:error:\n",
      "HTTPSConnectionPool(host='news.search.yandex.ru', port=443): Max retries exceeded with url: /search?text=trump&rpt=nnews2&grhow=clutop&rel=tm (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fc781383a00>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 54\n",
      "INFO:aiknows.ai:estimate prompt token length: 3279\n",
      "INFO:aiknows.ai:real prompt token length: 3550\n",
      "INFO:aiknows.ai:code:\n",
      "# it seems like the request failed, let's check the response status code\n",
      "response.status_code\n",
      "INFO:aiknows.ai:error:\n",
      "name 'response' is not defined\n"
     ]
    }
   ],
   "source": [
    "ai('get the 3 top news about trump from yandex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.ai:code:\n",
      "def filter_third(lst):\n",
      "    return lst[::3]\n",
      "\n",
      "# list isn't provided so we return a function that applies to it\n",
      "finish_task_ok(filter_third);\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function filter_third(lst)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('filter every third element out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 46\n",
      "INFO:aiknows.ai:estimate prompt token length: 2359\n",
      "INFO:aiknows.ai:real prompt token length: 2666\n",
      "INFO:aiknows.ai:code:\n",
      "# the task has no file args passed in\n",
      "# so we construct a function that takes a file path as input\n",
      "def clean_file(file_path):\n",
      "    with open(file_path, 'r') as f:\n",
      "        content = f.read()\n",
      "\n",
      "    # remove all whitespaces\n",
      "    content = ''.join(content.split())\n",
      "\n",
      "    with open(file_path, 'w') as f:\n",
      "        f.write(content)\n",
      "\n",
      "    return 'file cleaned'\n",
      "\n",
      "finish_task_ok(clean_file);\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function clean_file(file_path)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('read a file, clean all whitespaces and write back')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = [\n",
    "  {\n",
    "    'name' : 'alice',\n",
    "    'age (y)' : 22,\n",
    "    'id' : 92345\n",
    "  },\n",
    "  {\n",
    "    'name' : 'bob',\n",
    "    'age (y)' : 24,\n",
    "    'id' : 52353\n",
    "  },\n",
    "  {\n",
    "    'name' : 'tom',\n",
    "    'age (y)' : 30,\n",
    "    'id' : 62257\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = [\n",
    "  {\n",
    "    'name' : 'alice',\n",
    "    'age (y)' : 22,\n",
    "    'id' : 92345\n",
    "  },\n",
    "  {\n",
    "    'name' : 'bob',\n",
    "    'age (y)' : 24,\n",
    "    'id' : 52353\n",
    "  },\n",
    "  {\n",
    "    'name' : 'tom',\n",
    "    'age (y)' : 30,\n",
    "    'id' : 62257\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 44\n",
      "INFO:aiknows.ai:estimate prompt token length: 2193\n",
      "INFO:aiknows.ai:real prompt token length: 2539\n",
      "INFO:aiknows.ai:code:\n",
      "# we don't know how many matches are in a box, so we assume 50\n",
      "matches_in_box = 50\n",
      "\n",
      "# we don't know how many boxes are there, so we assume 10\n",
      "boxes = 10\n",
      "\n",
      "# we don't know how many matches are in total, so we calculate it\n",
      "total_matches = matches_in_box * boxes\n",
      "\n",
      "# we don't know how many matches are used on average, so we assume 5\n",
      "matches_used_on_average = 5\n",
      "\n",
      "# we don't know how many matches are left on average, so we calculate it\n",
      "matches_left_on_average = total_matches - matches_used_on_average\n",
      "\n",
      "finish_task_ok(matches_left_on_average);\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "495"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('how many matches in a box of matches on average?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 44\n",
      "INFO:aiknows.ai:estimate prompt token length: 2191\n",
      "INFO:aiknows.ai:real prompt token length: 2437\n",
      "INFO:aiknows.ai:code:\n",
      "import os\n",
      "\n",
      "# list all files in current directory\n",
      "files = os.listdir()\n",
      "\n",
      "# filter out directories\n",
      "files = [f for f in files if os.path.isfile(f)]\n",
      "\n",
      "finish_task_ok(files);\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['pyproject.toml',\n",
       " '.env.leave',\n",
       " 'README.md',\n",
       " '.gitignore',\n",
       " '.env',\n",
       " 'test.ipynb']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('print a list of files in current directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 44\n",
      "INFO:aiknows.ai:estimate prompt token length: 2189\n",
      "INFO:aiknows.ai:real prompt token length: 2433\n",
      "INFO:aiknows.ai:code:\n",
      "# two lists aren't provided so we return a function that applies to them\n",
      "def merge_lists(keys, values):\n",
      "    return dict(zip(keys, values))\n",
      "\n",
      "finish_task_ok(merge_lists);\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function merge_lists(keys, values)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('merge two lists into a dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.ai:code:\n",
      "# two lists are given, we assume they have same length\n",
      "keys = ['a', 'b', 'c']\n",
      "values = [1, 2, 3]\n",
      "\n",
      "# we can use zip to combine them into list of tuples\n",
      "# and then convert it to dict\n",
      "d = dict(zip(keys, values))\n",
      "\n",
      "finish_task_ok(d);\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a': 1, 'b': 2, 'c': 3}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('merge two lists indo dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 36\n",
      "INFO:aiknows.ai:estimate prompt token length: 1884\n",
      "INFO:aiknows.ai:real prompt token length: 2112\n",
      "INFO:aiknows.ai:code:\n",
      "# we don't know the structure of persons, so we print it out\n",
      "persons\n",
      "\n",
      "# we can't do anything with this, so we return a function\n",
      "def combined_age(persons):\n",
      "    return sum(p['age'] for p in persons)\n",
      "\n",
      "finish_task_ok(combined_age);\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function combined_age(persons)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('list of persons is given, return their combined age', persons=persons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I do not understand the question. Can you provide more context or information?\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt('kek vs mem? a?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 36\n",
      "INFO:aiknows.ai:estimate prompt token length: 1894\n",
      "INFO:aiknows.ai:real prompt token length: 2095\n",
      "INFO:aiknows.ai:code:\n",
      "# we assume that by a \\ b user meant set difference\n",
      "# both sets are provided so we just use them\n",
      "finish_task_ok(a - b);\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{4, 5}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('unique elements in a \\ b', a=set([1, 2, 3, 4, 5]), b=set([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 36\n",
      "INFO:aiknows.ai:estimate prompt token length: 1865\n",
      "INFO:aiknows.ai:real prompt token length: 2076\n",
      "INFO:aiknows.ai:code:\n",
      "def merge_lists_to_dict(keys, values):\n",
      "    return dict(zip(keys, values))\n",
      "\n",
      "# both lists are missing so we return a function that applies to them\n",
      "finish_task_ok(merge_lists_to_dict);\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: 'a', 2: 'b', 3: 'c'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('merge two lists indo dict')([1, 2, 3], ['a', 'b', 'c']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 36\n",
      "INFO:aiknows.ai:estimate prompt token length: 1879\n",
      "INFO:aiknows.ai:real prompt token length: 2154\n",
      "INFO:aiknows.ai:code:\n",
      "# we need to check if a number is prime\n",
      "def is_prime(n):\n",
      "    if n < 2:\n",
      "        return False\n",
      "    for i in range(2, int(n ** 0.5) + 1):\n",
      "        if n % i == 0:\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "# we need to filter out non-prime numbers from the list\n",
      "def filter_primes(l):\n",
      "    return [n for n in l if is_prime(n)]\n",
      "\n",
      "finish_task_ok(filter_primes(l));\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 3, 5, 7]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('filter non primes from l', l=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 36\n",
      "INFO:aiknows.ai:estimate prompt token length: 1832\n",
      "INFO:aiknows.ai:real prompt token length: 2063\n",
      "INFO:aiknows.ai:code:\n",
      "def filter_palindromes(words):\n",
      "    return [w for w in words if w != w[::-1]]\n",
      "\n",
      "# we assume that by filtering user meant filering from the list\n",
      "# list is not provided so we return a function that applies to it\n",
      "finish_task_ok(filter_palindromes);\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function filter_palindromes(words)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('filter palindromes out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 36\n",
      "INFO:aiknows.ai:estimate prompt token length: 1805\n",
      "INFO:aiknows.ai:real prompt token length: 2094\n",
      "INFO:aiknows.ai:code:\n",
      "# we don't know what primes are, so we define a function that checks if a number is prime\n",
      "def is_prime(n):\n",
      "    if n < 2:\n",
      "        return False\n",
      "    for i in range(2, int(n ** 0.5) + 1):\n",
      "        if n % i == 0:\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "# we assume that by filtering user meant filering from the list\n",
      "# list is provided so we reference it directly\n",
      "filtered_l = [x for x in l if is_prime(x)]\n",
      "\n",
      "finish_task_ok(filtered_l);\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 3, 5, 7]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('filter non primes from l', l=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 38\n",
      "INFO:aiknows.ai:estimate prompt token length: 1809\n",
      "INFO:aiknows.ai:real prompt token length: 2044\n",
      "INFO:aiknows.ai:code:\n",
      "def filter_non_strings(lst):\n",
      "    return [x for x in lst if isinstance(x, str)]\n",
      "\n",
      "# we assume that by filtering user meant filering from the list\n",
      "# list isn't provided so we return a function that applies to it\n",
      "finish_task_ok(filter_non_strings);\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('filter non strings from a list')([1, 'a', None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 36\n",
      "INFO:aiknows.ai:estimate prompt token length: 1804\n",
      "INFO:aiknows.ai:real prompt token length: 2014\n",
      "INFO:aiknows.ai:code:\n",
      "# we can reuse gpt function from previous tasks\n",
      "content_fr = gpt(f'Translate the following text to French: \"\"\"\\n{s}\\n\"\"\"')\n",
      "\n",
      "finish_task_ok(content_fr);\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"Bonjour\"'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('translate s to french', s='hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 36\n",
      "INFO:aiknows.ai:estimate prompt token length: 1762\n",
      "INFO:aiknows.ai:real prompt token length: 1967\n",
      "INFO:aiknows.ai:code:\n",
      "# we don't know the key name, could be: response, body, result, etc.\n",
      "# no problem, let's print them out and explore\n",
      "d.keys()\n",
      "INFO:aiknows.ai:result:\n",
      "dict_keys(['Q', 'A'])\n",
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 38\n",
      "INFO:aiknows.ai:estimate prompt token length: 1814\n",
      "INFO:aiknows.ai:real prompt token length: 2018\n",
      "INFO:aiknows.ai:code:\n",
      "# only 2 of them and 'A' makes the most sense in task context\n",
      "finish_task_ok(d['A']);\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('return answer from d', d={'Q': 'bark', 'A': 'dog'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 38\n",
      "INFO:aiknows.ai:estimate prompt token length: 1767\n",
      "INFO:aiknows.ai:real prompt token length: 2011\n",
      "INFO:aiknows.ai:code:\n",
      "# we can reuse the code from previous task\n",
      "# seconds x minutes x hours x days x years\n",
      "age_seconds = 4.6e9 * 365.25 * 24 * 60 * 60\n",
      "finish_task_ok(age_seconds / (365.25 * 24 * 60 * 60));\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4600000000.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('how old is the sun in years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "df = load_iris()\n",
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils._bunch.Bunch"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'object of type sklearn.utils._bunch.Bunch'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aiknows.explain import explain\n",
    "\n",
    "explain(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.1, 4.5, 2.4, 1.2],\n",
       "       [5.9, 4. , 2.4, 1.2],\n",
       "       [5.7, 4.2, 2.3, 1.2],\n",
       "       [5.6, 4.1, 2.5, 1.2],\n",
       "       [6. , 4.6, 2.4, 1.2],\n",
       "       [6.4, 4.9, 2.7, 1.4],\n",
       "       [5.6, 4.4, 2.4, 1.3],\n",
       "       [6. , 4.4, 2.5, 1.2],\n",
       "       [5.4, 3.9, 2.4, 1.2],\n",
       "       [5.9, 4.1, 2.5, 1.1],\n",
       "       [6.4, 4.7, 2.5, 1.2],\n",
       "       [5.8, 4.4, 2.6, 1.2],\n",
       "       [5.8, 4. , 2.4, 1.1],\n",
       "       [5.3, 4. , 2.1, 1.1],\n",
       "       [6.8, 5. , 2.2, 1.2],\n",
       "       [6.7, 5.4, 2.5, 1.4],\n",
       "       [6.4, 4.9, 2.3, 1.4],\n",
       "       [6.1, 4.5, 2.4, 1.3],\n",
       "       [6.7, 4.8, 2.7, 1.3],\n",
       "       [6.1, 4.8, 2.5, 1.3],\n",
       "       [6.4, 4.4, 2.7, 1.2],\n",
       "       [6.1, 4.7, 2.5, 1.4],\n",
       "       [5.6, 4.6, 2. , 1.2],\n",
       "       [6.1, 4.3, 2.7, 1.5],\n",
       "       [5.8, 4.4, 2.9, 1.2],\n",
       "       [6. , 4. , 2.6, 1.2],\n",
       "       [6. , 4.4, 2.6, 1.4],\n",
       "       [6.2, 4.5, 2.5, 1.2],\n",
       "       [6.2, 4.4, 2.4, 1.2],\n",
       "       [5.7, 4.2, 2.6, 1.2],\n",
       "       [5.8, 4.1, 2.6, 1.2],\n",
       "       [6.4, 4.4, 2.5, 1.4],\n",
       "       [6.2, 5.1, 2.5, 1.1],\n",
       "       [6.5, 5.2, 2.4, 1.2],\n",
       "       [5.9, 4.1, 2.5, 1.2],\n",
       "       [6. , 4.2, 2.2, 1.2],\n",
       "       [6.5, 4.5, 2.3, 1.2],\n",
       "       [5.9, 4.6, 2.4, 1.1],\n",
       "       [5.4, 4. , 2.3, 1.2],\n",
       "       [6.1, 4.4, 2.5, 1.2],\n",
       "       [6. , 4.5, 2.3, 1.3],\n",
       "       [5.5, 3.3, 2.3, 1.3],\n",
       "       [5.4, 4.2, 2.3, 1.2],\n",
       "       [6. , 4.5, 2.6, 1.6],\n",
       "       [6.1, 4.8, 2.9, 1.4],\n",
       "       [5.8, 4. , 2.4, 1.3],\n",
       "       [6.1, 4.8, 2.6, 1.2],\n",
       "       [5.6, 4.2, 2.4, 1.2],\n",
       "       [6.3, 4.7, 2.5, 1.2],\n",
       "       [6. , 4.3, 2.4, 1.2],\n",
       "       [8. , 4.2, 5.7, 2.4],\n",
       "       [7.4, 4.2, 5.5, 2.5],\n",
       "       [7.9, 4.1, 5.9, 2.5],\n",
       "       [6.5, 3.3, 5. , 2.3],\n",
       "       [7.5, 3.8, 5.6, 2.5],\n",
       "       [6.7, 3.8, 5.5, 2.3],\n",
       "       [7.3, 4.3, 5.7, 2.6],\n",
       "       [5.9, 3.4, 4.3, 2. ],\n",
       "       [7.6, 3.9, 5.6, 2.3],\n",
       "       [6.2, 3.7, 4.9, 2.4],\n",
       "       [6. , 3. , 4.5, 2. ],\n",
       "       [6.9, 4. , 5.2, 2.5],\n",
       "       [7. , 3.2, 5. , 2. ],\n",
       "       [7.1, 3.9, 5.7, 2.4],\n",
       "       [6.6, 3.9, 4.6, 2.3],\n",
       "       [7.7, 4.1, 5.4, 2.4],\n",
       "       [6.6, 4. , 5.5, 2.5],\n",
       "       [6.8, 3.7, 5.1, 2. ],\n",
       "       [7.2, 3.2, 5.5, 2.5],\n",
       "       [6.6, 3.5, 4.9, 2.1],\n",
       "       [6.9, 4.2, 5.8, 2.8],\n",
       "       [7.1, 3.8, 5. , 2.3],\n",
       "       [7.3, 3.5, 5.9, 2.5],\n",
       "       [7.1, 3.8, 5.7, 2.2],\n",
       "       [7.4, 3.9, 5.3, 2.3],\n",
       "       [7.6, 4. , 5.4, 2.4],\n",
       "       [7.8, 3.8, 5.8, 2.4],\n",
       "       [7.7, 4. , 6. , 2.7],\n",
       "       [7. , 3.9, 5.5, 2.5],\n",
       "       [6.7, 3.6, 4.5, 2. ],\n",
       "       [6.5, 3.4, 4.8, 2.1],\n",
       "       [6.5, 3.4, 4.7, 2. ],\n",
       "       [6.8, 3.7, 4.9, 2.2],\n",
       "       [7. , 3.7, 6.1, 2.6],\n",
       "       [6.4, 4. , 5.5, 2.5],\n",
       "       [7. , 4.4, 5.5, 2.6],\n",
       "       [7.7, 4.1, 5.7, 2.5],\n",
       "       [7.3, 3.3, 5.4, 2.3],\n",
       "       [6.6, 4. , 5.1, 2.3],\n",
       "       [6.5, 3.5, 5. , 2.3],\n",
       "       [6.5, 3.6, 5.4, 2.2],\n",
       "       [7.1, 4. , 5.6, 2.4],\n",
       "       [6.8, 3.6, 5. , 2.2],\n",
       "       [6. , 3.3, 4.3, 2. ],\n",
       "       [6.6, 3.7, 5.2, 2.3],\n",
       "       [6.7, 4. , 5.2, 2.2],\n",
       "       [6.7, 3.9, 5.2, 2.3],\n",
       "       [7.2, 3.9, 5.3, 2.3],\n",
       "       [6.1, 3.5, 4. , 2.1],\n",
       "       [6.7, 3.8, 5.1, 2.3],\n",
       "       [7.3, 4.3, 7. , 3.5],\n",
       "       [6.8, 3.7, 6.1, 2.9],\n",
       "       [8.1, 4. , 6.9, 3.1],\n",
       "       [7.3, 3.9, 6.6, 2.8],\n",
       "       [7.5, 4. , 6.8, 3.2],\n",
       "       [8.6, 4. , 7.6, 3.1],\n",
       "       [5.9, 3.5, 5.5, 2.7],\n",
       "       [8.3, 3.9, 7.3, 2.8],\n",
       "       [7.7, 3.5, 6.8, 2.8],\n",
       "       [8.2, 4.6, 7.1, 3.5],\n",
       "       [7.5, 4.2, 6.1, 3. ],\n",
       "       [7.4, 3.7, 6.3, 2.9],\n",
       "       [7.8, 4. , 6.5, 3.1],\n",
       "       [6.7, 3.5, 6. , 3. ],\n",
       "       [6.8, 3.8, 6.1, 3.4],\n",
       "       [7.4, 4.2, 6.3, 3.3],\n",
       "       [7.5, 4. , 6.5, 2.8],\n",
       "       [8.7, 4.8, 7.7, 3.2],\n",
       "       [8.7, 3.6, 7.9, 3.3],\n",
       "       [7. , 3.2, 6. , 2.5],\n",
       "       [7.9, 4.2, 6.7, 3.3],\n",
       "       [6.6, 3.8, 5.9, 3. ],\n",
       "       [8.7, 3.8, 7.7, 3. ],\n",
       "       [7.3, 3.7, 5.9, 2.8],\n",
       "       [7.7, 4.3, 6.7, 3.1],\n",
       "       [8.2, 4.2, 7. , 2.8],\n",
       "       [7.2, 3.8, 5.8, 2.8],\n",
       "       [7.1, 4. , 5.9, 2.8],\n",
       "       [7.4, 3.8, 6.6, 3.1],\n",
       "       [8.2, 4. , 6.8, 2.6],\n",
       "       [8.4, 3.8, 7.1, 2.9],\n",
       "       [8.9, 4.8, 7.4, 3. ],\n",
       "       [7.4, 3.8, 6.6, 3.2],\n",
       "       [7.3, 3.8, 6.1, 2.5],\n",
       "       [7.1, 3.6, 6.6, 2.4],\n",
       "       [8.7, 4. , 7.1, 3.3],\n",
       "       [7.3, 4.4, 6.6, 3.4],\n",
       "       [7.4, 4.1, 6.5, 2.8],\n",
       "       [7. , 4. , 5.8, 2.8],\n",
       "       [7.9, 4.1, 6.4, 3.1],\n",
       "       [7.7, 4.1, 6.6, 3.4],\n",
       "       [7.9, 4.1, 6.1, 3.3],\n",
       "       [6.8, 3.7, 6.1, 2.9],\n",
       "       [7.8, 4.2, 6.9, 3.3],\n",
       "       [7.7, 4.3, 6.7, 3.5],\n",
       "       [7.7, 4. , 6.2, 3.3],\n",
       "       [7.3, 3.5, 6. , 2.9],\n",
       "       [7.5, 4. , 6.2, 3. ],\n",
       "       [7.2, 4.4, 6.4, 3.3],\n",
       "       [6.9, 4. , 6.1, 2.8]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['data'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2, 4.9, 3. , 1.4, 0.2, 4.7, 3.2, 1.3, 0.2, 4.6,\n",
       "        3.1, 1.5, 0.2, 5. , 3.6, 1.4, 0.2, 5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3, 5. , 3.4, 1.5, 0.2, 4.4, 2.9, 1.4, 0.2, 4.9,\n",
       "        3.1, 1.5, 0.1, 5.4, 3.7, 1.5, 0.2, 4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1, 4.3, 3. , 1.1, 0.1, 5.8, 4. , 1.2, 0.2, 5.7,\n",
       "        4.4, 1.5, 0.4, 5.4, 3.9, 1.3, 0.4, 5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3, 5.1, 3.8, 1.5, 0.3, 5.4, 3.4, 1.7, 0.2, 5.1,\n",
       "        3.7, 1.5, 0.4, 4.6, 3.6, 1. , 0.2, 5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2, 5. , 3. , 1.6, 0.2, 5. , 3.4, 1.6, 0.4, 5.2,\n",
       "        3.5, 1.5, 0.2, 5.2, 3.4, 1.4, 0.2, 4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2, 5.4, 3.4, 1.5, 0.4, 5.2, 4.1, 1.5, 0.1, 5.5,\n",
       "        4.2, 1.4, 0.2, 4.9, 3.1, 1.5, 0.2, 5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2, 4.9, 3.6, 1.4, 0.1, 4.4, 3. , 1.3, 0.2, 5.1,\n",
       "        3.4, 1.5, 0.2, 5. , 3.5, 1.3, 0.3, 4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2, 5. , 3.5, 1.6, 0.6, 5.1, 3.8, 1.9, 0.4, 4.8,\n",
       "        3. , 1.4, 0.3, 5.1, 3.8, 1.6, 0.2, 4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2, 5. , 3.3, 1.4, 0.2, 7. , 3.2, 4.7, 1.4, 6.4,\n",
       "        3.2, 4.5, 1.5, 6.9, 3.1, 4.9, 1.5, 5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5, 5.7, 2.8, 4.5, 1.3, 6.3, 3.3, 4.7, 1.6, 4.9,\n",
       "        2.4, 3.3, 1. , 6.6, 2.9, 4.6, 1.3, 5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. , 5.9, 3. , 4.2, 1.5, 6. , 2.2, 4. , 1. , 6.1,\n",
       "        2.9, 4.7, 1.4, 5.6, 2.9, 3.6, 1.3, 6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5, 5.8, 2.7, 4.1, 1. , 6.2, 2.2, 4.5, 1.5, 5.6,\n",
       "        2.5, 3.9, 1.1, 5.9, 3.2, 4.8, 1.8, 6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5, 6.1, 2.8, 4.7, 1.2, 6.4, 2.9, 4.3, 1.3, 6.6,\n",
       "        3. , 4.4, 1.4, 6.8, 2.8, 4.8, 1.4, 6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5, 5.7, 2.6, 3.5, 1. , 5.5, 2.4, 3.8, 1.1, 5.5,\n",
       "        2.4, 3.7, 1. , 5.8, 2.7, 3.9, 1.2, 6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5, 6. , 3.4, 4.5, 1.6, 6.7, 3.1, 4.7, 1.5, 6.3,\n",
       "        2.3, 4.4, 1.3, 5.6, 3. , 4.1, 1.3, 5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2, 6.1, 3. , 4.6, 1.4, 5.8, 2.6, 4. , 1.2, 5. ,\n",
       "        2.3, 3.3, 1. , 5.6, 2.7, 4.2, 1.3, 5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3, 6.2, 2.9, 4.3, 1.3, 5.1, 2.5, 3. , 1.1, 5.7,\n",
       "        2.8, 4.1, 1.3, 6.3, 3.3, 6. , 2.5, 5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1, 6.3, 2.9, 5.6, 1.8, 6.5, 3. , 5.8, 2.2, 7.6,\n",
       "        3. , 6.6, 2.1, 4.9, 2.5, 4.5, 1.7, 7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8, 7.2, 3.6, 6.1, 2.5, 6.5, 3.2, 5.1, 2. , 6.4,\n",
       "        2.7, 5.3, 1.9, 6.8, 3. , 5.5, 2.1, 5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4, 6.4, 3.2, 5.3, 2.3, 6.5, 3. , 5.5, 1.8, 7.7,\n",
       "        3.8, 6.7, 2.2, 7.7, 2.6, 6.9, 2.3, 6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3, 5.6, 2.8, 4.9, 2. , 7.7, 2.8, 6.7, 2. , 6.3,\n",
       "        2.7, 4.9, 1.8, 6.7, 3.3, 5.7, 2.1, 7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8, 6.1, 3. , 4.9, 1.8, 6.4, 2.8, 5.6, 2.1, 7.2,\n",
       "        3. , 5.8, 1.6, 7.4, 2.8, 6.1, 1.9, 7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2, 6.3, 2.8, 5.1, 1.5, 6.1, 2.6, 5.6, 1.4, 7.7,\n",
       "        3. , 6.1, 2.3, 6.3, 3.4, 5.6, 2.4, 6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8, 6.9, 3.1, 5.4, 2.1, 6.7, 3.1, 5.6, 2.4, 6.9,\n",
       "        3.1, 5.1, 2.3, 5.8, 2.7, 5.1, 1.9, 6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5, 6.7, 3. , 5.2, 2.3, 6.3, 2.5, 5. , 1.9, 6.5,\n",
       "        3. , 5.2, 2. , 6.2, 3.4, 5.4, 2.3, 5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['data'].reshape(25, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 34\n",
      "INFO:aiknows.ai:estimate prompt token length: 1473\n",
      "INFO:aiknows.ai:real prompt token length: 1750\n",
      "INFO:aiknows.ai:assistant: ```python\n",
      "# packages might be missing, but we don't attempt to install them\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# calculate some statistics\n",
      "mean = np.mean(data)\n",
      "std = np.std(data)\n",
      "\n",
      "# create a histogram of the data\n",
      "fig, ax = plt.subplots()\n",
      "ax.hist(data.flatten(), bins=50)\n",
      "\n",
      "# add known labels and title to the plot\n",
      "ax.set_xlabel('Value')\n",
      "ax.set_ylabel('Frequency')\n",
      "ax.set_title('Histogram of Data')\n",
      "\n",
      "# user don't intend to observe outside effect\n",
      "plt.close(fig)\n",
      "\n",
      "final_result = fig\n",
      "```\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai('explore data and visialize it', data=df['data'].reshape(25, -1) + 1.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ai`, `aai`\n",
    "`gpt`, `agpt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiknows.ai:cache miss, generating bytecode\n",
      "INFO:aiknows.ai:len prompt messages: 34\n",
      "INFO:aiknows.ai:estimate prompt token length: 1462\n",
      "INFO:aiknows.ai:real prompt token length: 1675\n",
      "INFO:aiknows.ai:assistant: ```python\n",
      "import requests\n",
      "\n",
      "# make a GET request to the specified URL\n",
      "response = requests.get(url)\n",
      "\n",
      "# summarize the response using chat-gpt\n",
      "summary = gpt(f\"Summarize the following text: \\n{response.text}\")\n",
      "\n",
      "final_result = summary\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The text includes various pieces of information about cats, including the number of sounds they make (about 100), the amount of time they spend sleeping (70%), and the popularity of cats as pets in the United States (more than dogs). One fact also mentions the technical term for a cat's hairball (bezoar). One contributor says they don't know anything about cats.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://cat-fact.herokuapp.com/facts'\n",
    "ai('make a GET request to url and summarize the response', url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# we use duckduckgo's html api to get search results\n",
    "url = 'https://duckduckgo.com/html/'\n",
    "params = {\n",
    "    'q': 'trump news',\n",
    "    'kl': 'us-en',\n",
    "    't': 'h_',\n",
    "    'va': 'm',\n",
    "    'df': 'd',\n",
    "    's': '0',\n",
    "    'nextParams': '',\n",
    "    'v': 'l',\n",
    "    'o': 'json',\n",
    "    'api': '/d.js',\n",
    "}\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',\n",
    "    'Referer': 'https://duckduckgo.com/html/',\n",
    "}\n",
    "response = requests.get(url, params=params, headers=headers)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "news_headlines = [headline.text for headline in soup.select('.result__title')]\n",
    "top_3_news = news_headlines[:3]\n",
    "\n",
    "news_headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\\n\\n<!--[if IE 6]><html class=\"ie6\" xmlns=\"http://www.w3.org/1999/xhtml\"><![endif]-->\\n<!--[if IE 7]><html class=\"lt-ie8 lt-ie9\" xmlns=\"http://www.w3.org/1999/xhtml\"><![endif]-->\\n<!--[if IE 8]><html class=\"lt-ie9\" xmlns=\"http://www.w3.org/1999/xhtml\"><![endif]-->\\n<!--[if gt IE 8]><!--><html xmlns=\"http://www.w3.org/1999/xhtml\"><!--<![endif]-->\\n<head>\\n  <meta http-equiv=\"content-type\" content=\"text/html; charset=UTF-8\" />\\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, maximum-scale=3.0, user-scalable=1\" />\\n  <meta name=\"referrer\" content=\"origin\" />\\n  <meta name=\"HandheldFriendly\" content=\"true\" />\\n  <meta name=\"robots\" content=\"noindex, nofollow\" />\\n  <title>trump news at DuckDuckGo</title>\\n  <link title=\"DuckDuckGo (HTML)\" type=\"application/opensearchdescription+xml\" rel=\"search\" href=\"//duckduckgo.com/opensearch_html_v2.xml\" />\\n  <link href=\"//duckduckgo.com/favicon.ico\" rel=\"shortcut icon\" />\\n  <link rel=\"icon\" href=\"//duckduckgo.com/favicon.ico\" type=\"image/x-icon\" />\\n  <link rel=\"apple-touch-icon\" href=\"//duckduckgo.com/assets/logo_icon128.v101.png\"/>\\n  <link rel=\"image_src\" href=\"//duckduckgo.com/assets/logo_homepage.normal.v101.png\"/>\\n  <link type=\"text/css\" media=\"handheld, all\" href=\"//duckduckgo.com/h2153.css\" rel=\"stylesheet\" />\\n</head>\\n\\n<body class=\"body--html\">\\n  <a name=\"top\" id=\"top\"></a>\\n\\n  <form action=\"/html/\" method=\"post\">\\n    <input type=\"text\" name=\"state_hidden\" id=\"state_hidden\" />\\n  </form>\\n\\n  <div>\\n    <div class=\"site-wrapper-border\"></div>\\n\\n    <div id=\"header\" class=\"header cw header--html\">\\n        <a title=\"DuckDuckGo\" href=\"/html/\" class=\"header__logo-wrap\"></a>\\n\\n\\n    <form name=\"x\" class=\"header__form\" action=\"/html/\" method=\"post\">\\n\\n      <div class=\"search search--header\">\\n          <input name=\"q\" autocomplete=\"off\" class=\"search__input\" id=\"search_form_input_homepage\" type=\"text\" value=\"trump news\" />\\n          <input name=\"b\" id=\"search_button_homepage\" class=\"search__button search__button--html\" value=\"\" title=\"Search\" alt=\"Search\" type=\"submit\" />\\n      </div>\\n\\n\\n    \\n    \\n      <input name=\"t\" value=\"h_\" type=\"hidden\">\\n    \\n    \\n    \\n\\n    <div class=\"frm__select\">\\n      <select name=\"kl\">\\n      \\n        <option value=\"\" >All Regions</option>\\n      \\n        <option value=\"ar-es\" >Argentina</option>\\n      \\n        <option value=\"au-en\" >Australia</option>\\n      \\n        <option value=\"at-de\" >Austria</option>\\n      \\n        <option value=\"be-fr\" >Belgium (fr)</option>\\n      \\n        <option value=\"be-nl\" >Belgium (nl)</option>\\n      \\n        <option value=\"br-pt\" >Brazil</option>\\n      \\n        <option value=\"bg-bg\" >Bulgaria</option>\\n      \\n        <option value=\"ca-en\" >Canada (en)</option>\\n      \\n        <option value=\"ca-fr\" >Canada (fr)</option>\\n      \\n        <option value=\"ct-ca\" >Catalonia</option>\\n      \\n        <option value=\"cl-es\" >Chile</option>\\n      \\n        <option value=\"cn-zh\" >China</option>\\n      \\n        <option value=\"co-es\" >Colombia</option>\\n      \\n        <option value=\"hr-hr\" >Croatia</option>\\n      \\n        <option value=\"cz-cs\" >Czech Republic</option>\\n      \\n        <option value=\"dk-da\" >Denmark</option>\\n      \\n        <option value=\"ee-et\" >Estonia</option>\\n      \\n        <option value=\"fi-fi\" >Finland</option>\\n      \\n        <option value=\"fr-fr\" >France</option>\\n      \\n        <option value=\"de-de\" >Germany</option>\\n      \\n        <option value=\"gr-el\" >Greece</option>\\n      \\n        <option value=\"hk-tzh\" >Hong Kong</option>\\n      \\n        <option value=\"hu-hu\" >Hungary</option>\\n      \\n        <option value=\"is-is\" >Iceland</option>\\n      \\n        <option value=\"in-en\" >India (en)</option>\\n      \\n        <option value=\"id-en\" >Indonesia (en)</option>\\n      \\n        <option value=\"ie-en\" >Ireland</option>\\n      \\n        <option value=\"il-en\" >Israel (en)</option>\\n      \\n        <option value=\"it-it\" >Italy</option>\\n      \\n        <option value=\"jp-jp\" >Japan</option>\\n      \\n        <option value=\"kr-kr\" >Korea</option>\\n      \\n        <option value=\"lv-lv\" >Latvia</option>\\n      \\n        <option value=\"lt-lt\" >Lithuania</option>\\n      \\n        <option value=\"my-en\" >Malaysia (en)</option>\\n      \\n        <option value=\"mx-es\" >Mexico</option>\\n      \\n        <option value=\"nl-nl\" >Netherlands</option>\\n      \\n        <option value=\"nz-en\" >New Zealand</option>\\n      \\n        <option value=\"no-no\" >Norway</option>\\n      \\n        <option value=\"pk-en\" >Pakistan (en)</option>\\n      \\n        <option value=\"pe-es\" >Peru</option>\\n      \\n        <option value=\"ph-en\" >Philippines (en)</option>\\n      \\n        <option value=\"pl-pl\" >Poland</option>\\n      \\n        <option value=\"pt-pt\" >Portugal</option>\\n      \\n        <option value=\"ro-ro\" >Romania</option>\\n      \\n        <option value=\"ru-ru\" >Russia</option>\\n      \\n        <option value=\"xa-ar\" >Saudi Arabia</option>\\n      \\n        <option value=\"sg-en\" >Singapore</option>\\n      \\n        <option value=\"sk-sk\" >Slovakia</option>\\n      \\n        <option value=\"sl-sl\" >Slovenia</option>\\n      \\n        <option value=\"za-en\" >South Africa</option>\\n      \\n        <option value=\"es-ca\" >Spain (ca)</option>\\n      \\n        <option value=\"es-es\" >Spain (es)</option>\\n      \\n        <option value=\"se-sv\" >Sweden</option>\\n      \\n        <option value=\"ch-de\" >Switzerland (de)</option>\\n      \\n        <option value=\"ch-fr\" >Switzerland (fr)</option>\\n      \\n        <option value=\"tw-tzh\" >Taiwan</option>\\n      \\n        <option value=\"th-en\" >Thailand (en)</option>\\n      \\n        <option value=\"tr-tr\" >Turkey</option>\\n      \\n        <option value=\"us-en\" selected>US (English)</option>\\n      \\n        <option value=\"us-es\" >US (Spanish)</option>\\n      \\n        <option value=\"ua-uk\" >Ukraine</option>\\n      \\n        <option value=\"uk-en\" >United Kingdom</option>\\n      \\n        <option value=\"vn-en\" >Vietnam (en)</option>\\n      \\n      </select>\\n    </div>\\n\\n    <div class=\"frm__select frm__select--last\">\\n      <select class=\"\" name=\"df\">\\n      \\n        <option value=\"\" >Any Time</option>\\n      \\n        <option value=\"d\" selected>Past Day</option>\\n      \\n        <option value=\"w\" >Past Week</option>\\n      \\n        <option value=\"m\" >Past Month</option>\\n      \\n        <option value=\"y\" >Past Year</option>\\n      \\n      </select>\\n    </div>\\n\\n    </form>\\n\\n    </div>\\n\\n\\n\\n\\n\\n<!-- Web results are present -->\\n\\n  <div>\\n  <div class=\"serp__results\">\\n  <div id=\"links\" class=\"results\">\\n\\n      \\n\\n\\n\\n  \\n\\n\\n            <div class=\"result results_links results_links_deep web-result result--no-result\">\\n\\n\\n          <div class=\"links_main links_deep result__body\"> <!-- This is the visible part -->\\n\\n          <h2 class=\"result__title\">\\n          \\n          </h2>\\n\\n      \\n            <div class=\"no-results\">No results.</div>\\n      \\n\\n            <div class=\"result__extras\">\\n                <div class=\"result__extras__url\">\\n                  <span class=\"result__icon\">\\n                    \\n                  </span>\\n\\n                  <a class=\"result__url\" href=\"\">\\n                  \\n                  </a>\\n\\n                  \\n\\n                </div>\\n            </div>\\n\\n            \\n\\n            <div class=\"clear\"></div>\\n          </div>\\n\\n        </div>\\n\\n  \\n\\n\\n\\n\\n\\n        <div class=\" feedback-btn\">\\n            <a rel=\"nofollow\" href=\"//duckduckgo.com/feedback.html\" target=\"_new\">Feedback</a>\\n        </div>\\n        <div class=\"clear\"></div>\\n  </div>\\n  </div> <!-- links wrapper //-->\\n\\n\\n\\n    </div>\\n  </div>\\n\\n    <div id=\"bottom_spacing2\"></div>\\n\\n    \\n      <img src=\"//duckduckgo.com/t/sl_h\"/>\\n    \\n</body>\\n</html>\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiknows",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
